{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60C4j6xZS6Xh",
        "outputId": "1f1ba90b-6225-4efd-f56b-88b22d39866d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "import json"
      ],
      "metadata": {
        "id": "-kBABORZVn8R"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Train_data = \"/content/drive/MyDrive/data/train\"  # Update this path\n"
      ],
      "metadata": {
        "id": "syaGjR82Vtg8"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#TASK 1"
      ],
      "metadata": {
        "id": "v2q08_LYVw24"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_data(train_data_path, threshold=2):\n",
        "    word_freq, tag_freq, sentence_count = defaultdict(int), defaultdict(int), 0\n",
        "    file_data = [\"<s>\"]\n",
        "\n",
        "    with open(train_data_path) as f:\n",
        "        for line in f:\n",
        "            if line.strip() == \"\":\n",
        "                file_data.append(\"<s>\")\n",
        "                sentence_count += 1\n",
        "            else:\n",
        "                parts = line.strip().split(\"\\t\")\n",
        "                if len(parts) != 3:\n",
        "                    continue\n",
        "                word, tag = parts[1], parts[2]\n",
        "                word_freq[word] += 1\n",
        "                tag_freq[tag] += 1\n",
        "                file_data.append(line.strip())\n",
        "\n",
        "    vocab = {word: freq for word, freq in word_freq.items() if freq >= threshold}\n",
        "    vocab[\"<unk>\"] = sum(freq for word, freq in word_freq.items() if freq < threshold)\n",
        "\n",
        "    with open(\"vocab.txt\", \"w\") as vf:\n",
        "        vf.write(\"<unk>\\t0\\t{}\\n\".format(vocab[\"<unk>\"]))\n",
        "        for idx, (word, freq) in enumerate(sorted(vocab.items(), key=lambda x: x[1], reverse=True), start=1):\n",
        "            vf.write(\"{}\\t{}\\t{}\\n\".format(word, idx, freq))\n",
        "\n",
        "    print(f\"Vocabulary size after replacement: {len(vocab)}\")\n",
        "    print(f\"Total occurrences of ‘<unk>’: {vocab['<unk>']}\")\n",
        "\n",
        "    return file_data, vocab, tag_freq, sentence_count\n",
        "\n",
        "file_data, vocab, tag_freq, sentence_count = process_data(Train_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77GyxnLtVvBD",
        "outputId": "f7bb4666-5b96-4d8b-9c6a-819246dfca9b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size after replacement: 23183\n",
            "Total occurrences of ‘<unk>’: 20011\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#TASK 2"
      ],
      "metadata": {
        "id": "8iER1rOQV1kd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_hmm_parameters(file_data, vocab, tag_freq, sentence_count):\n",
        "    emission_probs, transition_probs = defaultdict(int), defaultdict(int)\n",
        "    prev_tag = \"<s>\"\n",
        "\n",
        "    for line in file_data:\n",
        "        if line == \"<s>\":\n",
        "            prev_tag = \"<s>\"\n",
        "            continue\n",
        "\n",
        "        parts = line.split(\"\\t\")\n",
        "        word, cur_tag = parts[1], parts[2]\n",
        "        word = word if word in vocab else \"<unk>\"\n",
        "        emission_probs[(cur_tag, word)] += 1\n",
        "\n",
        "        if prev_tag != \"<s>\":\n",
        "            transition_probs[(prev_tag, cur_tag)] += 1\n",
        "        else:\n",
        "            transition_probs[(\"start\", cur_tag)] += 1\n",
        "        prev_tag = cur_tag\n",
        "\n",
        "    for key in emission_probs:\n",
        "        emission_probs[key] /= tag_freq[key[0]]\n",
        "\n",
        "    for key in transition_probs:\n",
        "        if key[0] == \"start\":\n",
        "            transition_probs[key] /= sentence_count\n",
        "        else:\n",
        "            transition_probs[key] /= tag_freq[key[0]]\n",
        "\n",
        "\n",
        "    # Filter and print transition probabilities excluding \"start\"\n",
        "    filtered_transitions = {k: v for k, v in transition_probs.items() if k[0] != \"start\"}\n",
        "\n",
        "\n",
        "    print(f\"\\nNumber of emission parameters: {len(emission_probs)}\")\n",
        "    print(f\"Number of transition parameters: {len(transition_probs)}\")\n",
        "    print(f\"Number of transition parameters (excluding start): {len(filtered_transitions)}\")\n",
        "\n",
        "    return emission_probs, transition_probs\n"
      ],
      "metadata": {
        "id": "umwLQfZbV01H"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def save_hmm_model(emission_probs, transition_probs):\n",
        "    emission_str_keys = {f\"({tag},{word})\": prob for (tag, word), prob in emission_probs.items()}\n",
        "    transition_str_keys = {f\"({prev_tag},{next_tag})\": prob for (prev_tag, next_tag), prob in transition_probs.items()}\n",
        "\n",
        "    model = {\"Transition\": transition_str_keys, \"Emission\": emission_str_keys}\n",
        "    with open('hmm.json', 'w') as f:\n",
        "        json.dump(model, f, indent=4)\n",
        "\n",
        "    print(\"HMM model saved to hmm.json\")\n",
        "\n",
        "# Main execution\n",
        "\n",
        "emission_probs, transition_probs = compute_hmm_parameters(file_data, vocab, tag_freq, sentence_count)\n",
        "save_hmm_model(emission_probs, transition_probs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b32fvhrVVzzt",
        "outputId": "abfed2a4-60fc-4cd5-f4fd-aa261961f123"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of emission parameters: 30303\n",
            "Number of transition parameters: 1392\n",
            "Number of transition parameters (excluding start): 1351\n",
            "HMM model saved to hmm.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#TASK 3"
      ],
      "metadata": {
        "id": "nDg1Sz2f2-tu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tag_sentences_with_greedy(input_path, emissions, transitions, tag_set, vocab, data_type='dev'):\n",
        "    \"\"\"\n",
        "    Performs POS tagging on sentences using a greedy decoding algorithm.\n",
        "\n",
        "    Parameters:\n",
        "    - input_path: Path to the input file containing sentences.\n",
        "    - emissions: A dictionary with emission probabilities.\n",
        "    - transitions: A dictionary with transition probabilities.\n",
        "    - tag_set: A set containing all possible tags.\n",
        "    - vocab: A set containing all known vocabulary words.\n",
        "    - data_type: A string indicating the type of data ('dev' or 'test').\n",
        "    \"\"\"\n",
        "    # Choose the output file name based on data_type\n",
        "    output_file = \"greedy_dev.out\" if data_type == 'dev' else \"greedy.out\"\n",
        "\n",
        "    tagged_output = []  # Store the output lines here\n",
        "    current_tag = \"start\"  # Initialize the current tag as start\n",
        "\n",
        "    with open(input_path, 'r') as input_data:\n",
        "        for line in input_data:\n",
        "            if line.strip() == \"\":  # Sentence boundary detected\n",
        "                current_tag = \"start\"  # Reset for new sentence\n",
        "                tagged_output.append(\"\\n\")  # Keep sentences separated in output\n",
        "                continue\n",
        "\n",
        "            index, token = line.strip().split(\"\\t\")[:2]\n",
        "            processed_word = token if token in vocab else \"<unk>\"  # Handle unknown words\n",
        "\n",
        "            max_probability, chosen_tag = 0, None\n",
        "            for potential_tag in tag_set:\n",
        "                emission_key = (potential_tag, processed_word)\n",
        "                transition_key = (current_tag, potential_tag)\n",
        "\n",
        "                emission_probability = emissions.get(emission_key, 0)\n",
        "                transition_probability = transitions.get(transition_key, 0)\n",
        "                total_probability = emission_probability * transition_probability\n",
        "\n",
        "                if total_probability > max_probability:\n",
        "                    max_probability, chosen_tag = total_probability, potential_tag\n",
        "\n",
        "            current_tag = chosen_tag or \"start\"  # Update the current tag\n",
        "            tagged_output.append(f\"{index}\\t{token}\\t{chosen_tag}\\n\")  # Add the tagged line\n",
        "\n",
        "    with open(output_file, 'w') as output_data:\n",
        "        output_data.writelines(tagged_output)  # Write all tagged lines to the output file\n",
        "\n",
        "# Assuming tag_freq is a dictionary where keys are tags and values are their frequencies\n",
        "tags = set(tag_freq.keys())\n",
        "\n",
        "# Example calls\n",
        "tag_sentences_with_greedy(\"/content/drive/MyDrive/data/test\", emission_probs, transition_probs, tags, vocab, data_type='test')\n",
        "tag_sentences_with_greedy(\"/content/drive/MyDrive/data/dev\", emission_probs, transition_probs, tags, vocab, data_type='dev')\n"
      ],
      "metadata": {
        "id": "98mzzURPWB80"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"The accuracy on validation data using the greedy method is\")\n",
        "\n",
        "!python eval.py -p /content/greedy_dev.out -g /content/drive/MyDrive/data/dev\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "daAo1UKkebHW",
        "outputId": "08c0278c-88fb-4d9a-a344-c59e16cb7625"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The accuracy on validation data using the greedy method is\n",
            "total: 131768, correct: 123203, accuracy: 93.50%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def perform_viterbi_decoding(observed_words, state_list, emission_probabilities, transition_probabilities):\n",
        "    total_observations = len(observed_words)\n",
        "    total_states = len(state_list)\n",
        "    viterbi_matrix = [[0 for _ in range(total_states)] for _ in range(total_observations)]\n",
        "    backpointers = [[0 for _ in range(total_states)] for _ in range(total_observations)]\n",
        "\n",
        "    # Initialization step\n",
        "    for state_index in range(total_states):\n",
        "        transition_prob = transition_probabilities.get(('start', state_list[state_index]), 1e-10)\n",
        "        emission_prob = emission_probabilities.get((state_list[state_index], observed_words[0]), 1e-10)\n",
        "        viterbi_matrix[0][state_index] = transition_prob * emission_prob\n",
        "        backpointers[0][state_index] = 0\n",
        "\n",
        "    # Recursion step\n",
        "    for time_step in range(1, total_observations):\n",
        "        for state_index in range(total_states):\n",
        "            max_probability, best_previous_state = max(\n",
        "                (viterbi_matrix[time_step-1][prev_state] *\n",
        "                 transition_probabilities.get((state_list[prev_state], state_list[state_index]), 1e-10) *\n",
        "                 emission_probabilities.get((state_list[state_index], observed_words[time_step]), 1e-10), prev_state)\n",
        "                for prev_state in range(total_states))\n",
        "\n",
        "            viterbi_matrix[time_step][state_index] = max_probability\n",
        "            backpointers[time_step][state_index] = best_previous_state\n",
        "\n",
        "    # Termination step\n",
        "    last_time_step = total_observations - 1\n",
        "    best_final_state = max(range(total_states), key=lambda s: viterbi_matrix[last_time_step][s])\n",
        "\n",
        "    # Path backtracking\n",
        "    optimal_path = [best_final_state]\n",
        "    for time_step in range(total_observations - 1, 0, -1):\n",
        "        optimal_path.insert(0, backpointers[time_step][optimal_path[0]])\n",
        "\n",
        "    return [state_list[state] for state in optimal_path]\n",
        "\n",
        "\n",
        "# Assuming emission_probs is a dictionary where keys are tuples (tag, word)\n",
        "# Extract unique tags from the keys of emission_probs\n",
        "states = set(tag for tag, _ in emission_probs.keys())\n",
        "\n"
      ],
      "metadata": {
        "id": "wcCZBggSenTe"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_file_and_tag_viterbi(input_file_path, states, emission_probs, transition_probs, vocab, data_type='dev'):\n",
        "      # Determine the output file based on the data type\n",
        "\n",
        "    output_file = \"viterbi_dev.out\" if data_type == 'dev' else \"viterbi.out\"\n",
        "    viterbi_output = []\n",
        "\n",
        "    with open(input_file_path) as file:\n",
        "        lines = file.readlines()\n",
        "        word_indices, sentence_words, observed_words = [], [], []\n",
        "\n",
        "        for line in lines:\n",
        "            if len(line.strip()) == 0:  # Sentence boundary\n",
        "                if observed_words:\n",
        "                    tags = perform_viterbi_decoding(observed_words, states, emission_probs, transition_probs)\n",
        "                    viterbi_output.extend(f\"{index}\\t{word}\\t{tag}\\n\" for index, word, tag in zip(word_indices, sentence_words, tags))\n",
        "                # Reset for next sentence\n",
        "                observed_words, word_indices, sentence_words = [], [], []\n",
        "                viterbi_output.append(\"\\n\")\n",
        "                continue\n",
        "\n",
        "            index, word = line.strip().split(\"\\t\")[:2]\n",
        "            word_indices.append(index)\n",
        "            sentence_words.append(word)\n",
        "            observed_words.append(word if word in vocab else \"<unk>\")\n",
        "\n",
        "    with open(output_file, \"w\") as viterbi_file:\n",
        "        viterbi_file.writelines(viterbi_output)\n",
        "\n",
        "# Example usage\n",
        "dev_file_path = \"/content/drive/MyDrive/data/dev\"  # For development data\n",
        "test_file_path = \"/content/drive/MyDrive/data/test\"  # For test data\n",
        "\n",
        "# If 'states' was originally defined as a set, convert it to a list\n",
        "states_list = list(states)\n",
        "\n",
        "# Then, pass this list to the process_file_and_tag_viterbi function\n",
        "process_file_and_tag_viterbi(dev_file_path, states_list, emission_probs, transition_probs, vocab, data_type='dev')\n",
        "process_file_and_tag_viterbi(test_file_path, states_list, emission_probs, transition_probs, vocab, data_type='test')\n",
        "\n"
      ],
      "metadata": {
        "id": "oyoH4LoGkENJ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"The accuracy on validation data using the viterbi decoding method is\")\n",
        "\n",
        "!python eval.py -p /content/viterbi_dev.out -g /content/drive/MyDrive/data/dev\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CSgxYfUZkIcQ",
        "outputId": "1ab3d4d6-1e78-4a6a-99fe-9a6eee4c7c96"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The accuracy on validation data using the viterbi decoding method is\n",
            "'1\\tThat\\tDT' '38\\t.\\t.' 131751\n",
            "total: 131751, correct: 124912, accuracy: 94.81%\n"
          ]
        }
      ]
    }
  ]
}