{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8betADzkExH5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from collections import Counter\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_split = \"/content/drive/MyDrive/nlp_hw4(dataset)/train\"\n",
        "dev_split = \"/content/drive/MyDrive/nlp_hw4(dataset)/dev\"\n",
        "test_split = \"/content/drive/MyDrive/nlp_hw4(dataset)/test\""
      ],
      "metadata": {
        "id": "c2lrBPWWE0M_"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
      ],
      "metadata": {
        "id": "JyRrEydbGVyW"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Simple_BiLSTM(nn.Module):\n",
        "    def __init__(self, embedding_dim, hidden_dim, output_dim, dropout):\n",
        "        super(Simple_BiLSTM, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.cap_embedding = nn.Embedding(2, 50)\n",
        "        self.lstm = nn.LSTM(100, hidden_dim, batch_first=True, bidirectional=True)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc1 = nn.Linear(hidden_dim*2, 128)\n",
        "        self.activation = nn.ELU()\n",
        "        self.fc2 = nn.Linear(128, output_dim)\n",
        "\n",
        "\n",
        "    def forward(self, sentence):\n",
        "        embedded = self.embedding(sentence)\n",
        "        lstm_out, _ = self.lstm(embedded)\n",
        "        fc1_out = self.fc1(lstm_out)\n",
        "        activation_out = self.activation(fc1_out)\n",
        "        output = self.fc2(activation_out)\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "RBjvn2tLGe5a"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unk_threshold = 1\n",
        "PAD = '<PAD>'\n",
        "UNK = '<UNK>'\n",
        "BATCH_SIZE = 16"
      ],
      "metadata": {
        "id": "TVAMB0Mku4pL"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def DataPreprocess(file):\n",
        "    with open(file) as f:\n",
        "        all_str = f.read()\n",
        "        sentences = all_str.split(\"\\n\\n\")\n",
        "\n",
        "    sentences = [[i for i in sen.split(\"\\n\") if i] for sen in sentences]\n",
        "    words = [[line.split()[1] for line in sen] for sen in sentences]\n",
        "\n",
        "    # Count word frequencies\n",
        "    word_freqs = Counter([word for sentence in words for word in sentence])\n",
        "\n",
        "    # Filter out words with frequency <= unk_th\n",
        "    words_set = [word for word, freq in word_freqs.items() if freq > unk_threshold]\n",
        "\n",
        "    words = [[word if word_freqs[word] > unk_threshold else UNK for word in sen] for sen in words]\n",
        "\n",
        "    ners = [[line.split()[2] for line in sen] for sen in sentences]\n",
        "    ners_set = list(set([line.split()[2] for sen in sentences for line in sen]))\n",
        "\n",
        "    # Add PAD and UNK tokens to words_set\n",
        "    words_set = [PAD] + [UNK] + words_set\n",
        "\n",
        "    # Update ners_set with PAD token\n",
        "    ners_set = [PAD] + ners_set\n",
        "\n",
        "    words = [[word if word in words_set else UNK for word in sentence] for sentence in words]\n",
        "    return words, ners, words_set, ners_set"
      ],
      "metadata": {
        "id": "-UMf-xR2NJOc"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convertFileToTensor(input_file):\n",
        "    words, ners, words_set, ners_set = DataPreprocess(input_file)\n",
        "\n",
        "    word_encoder, tag_encoder = LabelEncoder(), LabelEncoder()\n",
        "\n",
        "    word_indices = word_encoder.fit_transform(words_set)\n",
        "    tag_indices = tag_encoder.fit_transform(ners_set)\n",
        "\n",
        "    word2idx = {}\n",
        "    for i, word in enumerate(words_set):\n",
        "        word2idx[word] = word_indices[i]\n",
        "\n",
        "    tag2idx = {}\n",
        "    for i, tag in enumerate(ners_set):\n",
        "        tag2idx[tag] = tag_indices[i]\n",
        "\n",
        "\n",
        "    # Convert the sentences to indices\n",
        "    sentences_word_indices = [[word2idx[word] for word in sentence] for sentence in words]\n",
        "    sentences_tag_indices = [[tag2idx[tag] for tag in sentence] for sentence in ners]\n",
        "\n",
        "    return sentences_word_indices, sentences_tag_indices, len(word2idx), len(tag2idx), word2idx, tag2idx"
      ],
      "metadata": {
        "id": "XyaLQ9NqNLnd"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convertFileToTensor_test(file, word2idx, tag2idx):\n",
        "    # Open the file and read all its contents into a single string\n",
        "    with open(file) as f:\n",
        "        all_str = f.read()\n",
        "\n",
        "    # Split the string into sentences based on double newline characters\n",
        "    sentences = all_str.split(\"\\n\\n\")\n",
        "    # Further split each sentence into lines, filtering out empty lines\n",
        "    sentences = [[i for i in sen.split(\"\\n\") if i] for sen in sentences]\n",
        "    # For each sentence, extract the words, which are assumed to be the second element on each line\n",
        "    words = [[line.split()[1] for line in sen] for sen in sentences]\n",
        "    # Similarly, extract the named entity tags, which are assumed to be the third element on each line\n",
        "    ners = [[line.split()[2] for line in sen] for sen in sentences]\n",
        "\n",
        "    # Convert each word in each sentence to its corresponding index in word2idx\n",
        "    # Use the index for UNK (unknown) if the word is not in the dictionary\n",
        "    sentences_word_indices = [\n",
        "        [word2idx[word] if word in word2idx else word2idx[UNK] for word in sentence]\n",
        "        for sentence in words\n",
        "    ]\n",
        "\n",
        "    # Convert each named entity tag in each sentence to its corresponding index in tag2idx\n",
        "    sentences_tag_indices = [\n",
        "        [tag2idx[tag] for tag in sentence] for sentence in ners\n",
        "    ]\n",
        "\n",
        "    # Return the list of word indices and the list of tag indices\n",
        "    return sentences_word_indices, sentences_tag_indices\n"
      ],
      "metadata": {
        "id": "b3ORzniPNN7_"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "    padded_word_indices = pad_sequence([b[0] for b in batch], batch_first=True, padding_value=0)\n",
        "    padded_tag_indices = pad_sequence([b[1] for b in batch], batch_first=True, padding_value=0)\n",
        "    return padded_word_indices, padded_tag_indices"
      ],
      "metadata": {
        "id": "usfUGNzaNUGS"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, x, y):\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.tensor(self.x[idx]), torch.tensor(self.y[idx])"
      ],
      "metadata": {
        "id": "kcPepwac97yu"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_padded_word_indices, train_padded_tag_indices, vocab_size, num_classes, word2idx, tag2idx = convertFileToTensor(train_split)\n",
        "train_dataset = CustomDataset(train_padded_word_indices, train_padded_tag_indices)\n",
        "train_loader = DataLoader(train_dataset, collate_fn=collate_fn, batch_size=BATCH_SIZE, shuffle=True)\n"
      ],
      "metadata": {
        "id": "tjE54aoFNXqs"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dev_padded_word_indices, dev_padded_tag_indices = convertFileToTensor_test(dev_split, word2idx, tag2idx)\n",
        "dev_dataset = CustomDataset(dev_padded_word_indices, dev_padded_tag_indices)\n",
        "dev_dataloader = DataLoader(dev_dataset, collate_fn=collate_fn, batch_size=BATCH_SIZE)\n"
      ],
      "metadata": {
        "id": "qvMwpb8-lceo"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Simple_BiLSTM(embedding_dim=100, hidden_dim=256, output_dim=10, dropout=0.33).to(device)\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GwYoxhUXli4u",
        "outputId": "1c7e61ed-cf31-4ea6-ee15-6a4dba1fd5a1"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Simple_BiLSTM(\n",
              "  (embedding): Embedding(11985, 100)\n",
              "  (cap_embedding): Embedding(2, 50)\n",
              "  (lstm): LSTM(100, 256, batch_first=True, bidirectional=True)\n",
              "  (dropout): Dropout(p=0.33, inplace=False)\n",
              "  (fc1): Linear(in_features=512, out_features=128, bias=True)\n",
              "  (activation): ELU(alpha=1.0)\n",
              "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# saving json files\n",
        "#for key, value in word2idx.items():\n",
        " # word2idx[key] = int(value)\n",
        "#with open(\"word2idx_task1.json\", \"w+\") as f:\n",
        " # json.dump(word2idx, f)\n",
        "#idx2word = {v:k for k, v in word2idx.items()}\n",
        "#with open(\"idx2word_task1.json\", \"w+\") as f:\n",
        " # json.dump(idx2word, f)\n",
        "#for key, value in tag2idx.items():\n",
        " # tag2idx[key] = int(value)\n",
        "#with open(\"tag2idx_task1.json\", \"w+\") as f:\n",
        " # json.dump(tag2idx, f)"
      ],
      "metadata": {
        "id": "q5BGdPDvNvzf"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss(ignore_index=tag2idx[PAD])\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.5, weight_decay=1e-5)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.9)\n",
        "num_epochs = 100"
      ],
      "metadata": {
        "id": "Hd-l4cTYNyoj"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "def calculate_f1(model, dataloader, device, tag2idx):\n",
        "    true_tags = []\n",
        "    predicted_tags = []\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in dataloader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            predictions = outputs.argmax(dim=2)\n",
        "\n",
        "            # Flatten the batch and remove PAD tokens\n",
        "            for i in range(inputs.size(0)):  # Loop over the batch\n",
        "                for j in range(inputs.size(1)):  # Loop over sequence length\n",
        "                    if targets[i, j] != tag2idx[PAD]:  # Check if it's not a PAD token\n",
        "                        true_tags.append(targets[i, j].item())\n",
        "                        predicted_tags.append(predictions[i, j].item())\n",
        "\n",
        "    return f1_score(true_tags, predicted_tags, average='macro')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JpXBUpgqla00"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Initialize variables to track the best F1 score\n",
        "best_f1 = 0.0\n",
        "\n",
        "# Training Loop\n",
        "for epoch in tqdm(range(num_epochs)):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        outputs = outputs.view(-1, num_classes)\n",
        "\n",
        "        loss = criterion(outputs, targets.view(-1))\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        # Implement gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "    scheduler.step()\n",
        "\n",
        "    # Calculate F1 score on development set\n",
        "    f1 = calculate_f1(model, dev_dataloader, device, tag2idx)\n",
        "\n",
        "    # Save model if F1 score is the best so far\n",
        "    if f1 > best_f1:\n",
        "        best_f1 = f1\n",
        "        torch.save(model.state_dict(), 'blstm1.pt')\n",
        "\n",
        "    # Print epoch metrics\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}: Loss={epoch_loss:.4f}, F1 Score on Dev Set: {f1:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9--Xz7Tlla4g",
        "outputId": "808d9447-6653-4d9f-aa5f-3039a336e7df"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|          | 1/100 [00:14<23:47, 14.42s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100: Loss=532.9920, F1 Score on Dev Set: 0.3938\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  2%|▏         | 2/100 [00:27<22:05, 13.53s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/100: Loss=307.5179, F1 Score on Dev Set: 0.6402\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  3%|▎         | 3/100 [00:39<20:54, 12.94s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/100: Loss=194.5874, F1 Score on Dev Set: 0.6969\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  4%|▍         | 4/100 [00:51<20:15, 12.66s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/100: Loss=129.1701, F1 Score on Dev Set: 0.7589\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  5%|▌         | 5/100 [01:04<19:49, 12.52s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/100: Loss=90.7765, F1 Score on Dev Set: 0.7290\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  6%|▌         | 6/100 [01:16<19:27, 12.42s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/100: Loss=65.8132, F1 Score on Dev Set: 0.7985\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  7%|▋         | 7/100 [01:28<19:11, 12.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/100: Loss=49.5241, F1 Score on Dev Set: 0.8098\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  8%|▊         | 8/100 [01:40<18:43, 12.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/100: Loss=37.9324, F1 Score on Dev Set: 0.8127\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  9%|▉         | 9/100 [01:57<20:52, 13.77s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/100: Loss=29.0048, F1 Score on Dev Set: 0.8188\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 10%|█         | 10/100 [02:09<19:55, 13.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/100: Loss=23.1377, F1 Score on Dev Set: 0.8111\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 11%|█         | 11/100 [02:22<19:12, 12.95s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11/100: Loss=17.0981, F1 Score on Dev Set: 0.8161\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 12%|█▏        | 12/100 [02:33<18:29, 12.61s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12/100: Loss=13.7783, F1 Score on Dev Set: 0.7872\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 13%|█▎        | 13/100 [02:45<17:58, 12.40s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13/100: Loss=11.3171, F1 Score on Dev Set: 0.8213\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 14%|█▍        | 14/100 [02:57<17:39, 12.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14/100: Loss=9.2952, F1 Score on Dev Set: 0.8233\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 15%|█▌        | 15/100 [03:11<17:51, 12.61s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15/100: Loss=7.4442, F1 Score on Dev Set: 0.8159\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 16%|█▌        | 16/100 [03:24<18:03, 12.90s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16/100: Loss=6.4483, F1 Score on Dev Set: 0.8129\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 17%|█▋        | 17/100 [03:36<17:31, 12.67s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17/100: Loss=6.5177, F1 Score on Dev Set: 0.8175\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 18%|█▊        | 18/100 [03:49<17:05, 12.51s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18/100: Loss=5.8737, F1 Score on Dev Set: 0.8202\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 19%|█▉        | 19/100 [04:01<16:47, 12.44s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19/100: Loss=4.7819, F1 Score on Dev Set: 0.8178\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|██        | 20/100 [04:13<16:30, 12.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20/100: Loss=4.6995, F1 Score on Dev Set: 0.8167\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 21%|██        | 21/100 [04:25<16:14, 12.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 21/100: Loss=3.9541, F1 Score on Dev Set: 0.8132\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 22%|██▏       | 22/100 [04:38<16:06, 12.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 22/100: Loss=3.2816, F1 Score on Dev Set: 0.8181\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 23%|██▎       | 23/100 [04:50<15:47, 12.30s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 23/100: Loss=3.5370, F1 Score on Dev Set: 0.8143\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 24%|██▍       | 24/100 [05:02<15:31, 12.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 24/100: Loss=2.9536, F1 Score on Dev Set: 0.8188\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 25%|██▌       | 25/100 [05:14<15:18, 12.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 25/100: Loss=3.0395, F1 Score on Dev Set: 0.8191\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 26%|██▌       | 26/100 [05:26<15:05, 12.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 26/100: Loss=2.8409, F1 Score on Dev Set: 0.8189\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 27%|██▋       | 27/100 [05:39<14:49, 12.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 27/100: Loss=2.7625, F1 Score on Dev Set: 0.8145\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 28%|██▊       | 28/100 [05:50<14:25, 12.03s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 28/100: Loss=2.8701, F1 Score on Dev Set: 0.8157\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 29%|██▉       | 29/100 [06:02<14:10, 11.98s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 29/100: Loss=2.7827, F1 Score on Dev Set: 0.8132\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 30%|███       | 30/100 [06:14<14:02, 12.03s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 30/100: Loss=2.6492, F1 Score on Dev Set: 0.8158\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 31%|███       | 31/100 [06:26<13:52, 12.07s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 31/100: Loss=2.4318, F1 Score on Dev Set: 0.8129\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 32%|███▏      | 32/100 [06:39<13:41, 12.08s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 32/100: Loss=2.1373, F1 Score on Dev Set: 0.8138\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 33%|███▎      | 33/100 [06:51<13:32, 12.12s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 33/100: Loss=2.0592, F1 Score on Dev Set: 0.8199\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 34%|███▍      | 34/100 [07:03<13:23, 12.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 34/100: Loss=2.1914, F1 Score on Dev Set: 0.8117\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 35%|███▌      | 35/100 [07:16<13:29, 12.46s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 35/100: Loss=2.1045, F1 Score on Dev Set: 0.8169\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 36%|███▌      | 36/100 [07:28<13:11, 12.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 36/100: Loss=2.0459, F1 Score on Dev Set: 0.8124\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 37%|███▋      | 37/100 [07:41<12:56, 12.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 37/100: Loss=1.8071, F1 Score on Dev Set: 0.8155\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 38%|███▊      | 38/100 [07:53<12:43, 12.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 38/100: Loss=1.9206, F1 Score on Dev Set: 0.8182\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 39%|███▉      | 39/100 [08:05<12:30, 12.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 39/100: Loss=1.9509, F1 Score on Dev Set: 0.8113\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|████      | 40/100 [08:17<12:14, 12.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 40/100: Loss=2.2300, F1 Score on Dev Set: 0.8202\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 41%|████      | 41/100 [08:29<12:02, 12.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 41/100: Loss=1.9133, F1 Score on Dev Set: 0.8191\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 42%|████▏     | 42/100 [08:42<11:46, 12.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 42/100: Loss=1.9346, F1 Score on Dev Set: 0.8163\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 43%|████▎     | 43/100 [08:53<11:28, 12.07s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 43/100: Loss=1.8584, F1 Score on Dev Set: 0.8178\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 44%|████▍     | 44/100 [09:05<11:13, 12.03s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 44/100: Loss=1.6527, F1 Score on Dev Set: 0.8176\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 45%|████▌     | 45/100 [09:17<11:04, 12.08s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 45/100: Loss=1.7876, F1 Score on Dev Set: 0.8159\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 46%|████▌     | 46/100 [09:30<10:53, 12.11s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 46/100: Loss=1.6978, F1 Score on Dev Set: 0.8187\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 47%|████▋     | 47/100 [09:42<10:43, 12.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 47/100: Loss=1.8309, F1 Score on Dev Set: 0.8138\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 48%|████▊     | 48/100 [09:54<10:32, 12.16s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 48/100: Loss=1.7795, F1 Score on Dev Set: 0.8139\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 49%|████▉     | 49/100 [10:06<10:22, 12.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 49/100: Loss=1.7984, F1 Score on Dev Set: 0.7872\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|█████     | 50/100 [10:18<10:09, 12.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 50/100: Loss=1.8347, F1 Score on Dev Set: 0.8148\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 51%|█████     | 51/100 [10:31<09:56, 12.18s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 51/100: Loss=1.6725, F1 Score on Dev Set: 0.8139\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 52%|█████▏    | 52/100 [10:43<09:44, 12.18s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 52/100: Loss=1.6726, F1 Score on Dev Set: 0.8160\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 53%|█████▎    | 53/100 [10:55<09:31, 12.16s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 53/100: Loss=1.6142, F1 Score on Dev Set: 0.8167\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 54%|█████▍    | 54/100 [11:07<09:18, 12.15s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 54/100: Loss=1.6339, F1 Score on Dev Set: 0.8116\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 55%|█████▌    | 55/100 [11:20<09:17, 12.38s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 55/100: Loss=1.8342, F1 Score on Dev Set: 0.8141\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 56%|█████▌    | 56/100 [11:32<09:02, 12.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 56/100: Loss=1.6333, F1 Score on Dev Set: 0.8155\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 57%|█████▋    | 57/100 [11:44<08:47, 12.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 57/100: Loss=1.7589, F1 Score on Dev Set: 0.7989\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 58%|█████▊    | 58/100 [11:56<08:31, 12.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 58/100: Loss=1.6356, F1 Score on Dev Set: 0.8078\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 59%|█████▉    | 59/100 [12:08<08:19, 12.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 59/100: Loss=1.5255, F1 Score on Dev Set: 0.8050\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|██████    | 60/100 [12:21<08:05, 12.15s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 60/100: Loss=1.5923, F1 Score on Dev Set: 0.8133\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 61%|██████    | 61/100 [12:33<07:55, 12.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 61/100: Loss=1.5365, F1 Score on Dev Set: 0.8170\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 62%|██████▏   | 62/100 [12:45<07:44, 12.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 62/100: Loss=1.6531, F1 Score on Dev Set: 0.8188\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 63%|██████▎   | 63/100 [12:57<07:30, 12.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 63/100: Loss=1.5400, F1 Score on Dev Set: 0.8163\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 64%|██████▍   | 64/100 [13:10<07:19, 12.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 64/100: Loss=1.6485, F1 Score on Dev Set: 0.8135\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 65%|██████▌   | 65/100 [13:22<07:08, 12.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 65/100: Loss=1.5564, F1 Score on Dev Set: 0.8145\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 66%|██████▌   | 66/100 [13:34<06:57, 12.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 66/100: Loss=1.5489, F1 Score on Dev Set: 0.8158\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 67%|██████▋   | 67/100 [13:46<06:44, 12.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 67/100: Loss=1.6814, F1 Score on Dev Set: 0.8142\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 68%|██████▊   | 68/100 [13:59<06:33, 12.30s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 68/100: Loss=1.5594, F1 Score on Dev Set: 0.8133\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 69%|██████▉   | 69/100 [14:11<06:22, 12.34s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 69/100: Loss=1.5784, F1 Score on Dev Set: 0.8168\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 70%|███████   | 70/100 [14:24<06:11, 12.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 70/100: Loss=1.6098, F1 Score on Dev Set: 0.8149\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 71%|███████   | 71/100 [14:36<05:59, 12.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 71/100: Loss=1.5479, F1 Score on Dev Set: 0.8123\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 72%|███████▏  | 72/100 [14:51<06:10, 13.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 72/100: Loss=1.4710, F1 Score on Dev Set: 0.8145\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 73%|███████▎  | 73/100 [15:06<06:08, 13.65s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 73/100: Loss=1.4930, F1 Score on Dev Set: 0.8179\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 74%|███████▍  | 74/100 [15:19<05:52, 13.54s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 74/100: Loss=1.6561, F1 Score on Dev Set: 0.8127\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 75%|███████▌  | 75/100 [15:32<05:29, 13.18s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 75/100: Loss=1.4880, F1 Score on Dev Set: 0.8136\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 76%|███████▌  | 76/100 [15:44<05:10, 12.95s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 76/100: Loss=1.6150, F1 Score on Dev Set: 0.8135\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 77%|███████▋  | 77/100 [15:56<04:53, 12.76s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 77/100: Loss=1.6510, F1 Score on Dev Set: 0.8147\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 78%|███████▊  | 78/100 [16:09<04:38, 12.64s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 78/100: Loss=1.5340, F1 Score on Dev Set: 0.8143\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 79%|███████▉  | 79/100 [16:21<04:23, 12.55s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 79/100: Loss=1.5448, F1 Score on Dev Set: 0.8156\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|████████  | 80/100 [16:33<04:09, 12.49s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 80/100: Loss=1.4458, F1 Score on Dev Set: 0.8035\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 81%|████████  | 81/100 [16:45<03:53, 12.30s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 81/100: Loss=1.4581, F1 Score on Dev Set: 0.8145\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 82%|████████▏ | 82/100 [16:57<03:39, 12.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 82/100: Loss=1.4806, F1 Score on Dev Set: 0.8130\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 83%|████████▎ | 83/100 [17:09<03:26, 12.15s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 83/100: Loss=1.5123, F1 Score on Dev Set: 0.8131\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 84%|████████▍ | 84/100 [17:23<03:23, 12.74s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 84/100: Loss=1.5501, F1 Score on Dev Set: 0.8143\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 85%|████████▌ | 85/100 [17:35<03:07, 12.53s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 85/100: Loss=1.5351, F1 Score on Dev Set: 0.8061\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 86%|████████▌ | 86/100 [17:47<02:53, 12.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 86/100: Loss=1.5021, F1 Score on Dev Set: 0.8050\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 87%|████████▋ | 87/100 [18:00<02:40, 12.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 87/100: Loss=1.5700, F1 Score on Dev Set: 0.8113\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 88%|████████▊ | 88/100 [18:12<02:29, 12.49s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 88/100: Loss=1.4095, F1 Score on Dev Set: 0.8126\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 89%|████████▉ | 89/100 [18:25<02:16, 12.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 89/100: Loss=1.4402, F1 Score on Dev Set: 0.8123\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 90%|█████████ | 90/100 [18:37<02:03, 12.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 90/100: Loss=1.5333, F1 Score on Dev Set: 0.8014\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 91%|█████████ | 91/100 [18:49<01:50, 12.29s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 91/100: Loss=1.4391, F1 Score on Dev Set: 0.8114\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 92%|█████████▏| 92/100 [19:01<01:38, 12.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 92/100: Loss=1.4135, F1 Score on Dev Set: 0.8143\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 93%|█████████▎| 93/100 [19:14<01:27, 12.52s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 93/100: Loss=1.5547, F1 Score on Dev Set: 0.8130\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 94%|█████████▍| 94/100 [19:27<01:14, 12.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 94/100: Loss=1.5560, F1 Score on Dev Set: 0.8112\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 95%|█████████▌| 95/100 [19:39<01:01, 12.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 95/100: Loss=1.4287, F1 Score on Dev Set: 0.8096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 96%|█████████▌| 96/100 [19:51<00:49, 12.30s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 96/100: Loss=1.4326, F1 Score on Dev Set: 0.8115\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 97%|█████████▋| 97/100 [20:03<00:36, 12.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 97/100: Loss=1.5018, F1 Score on Dev Set: 0.8110\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 98%|█████████▊| 98/100 [20:15<00:24, 12.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 98/100: Loss=1.4925, F1 Score on Dev Set: 0.8107\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 99%|█████████▉| 99/100 [20:27<00:12, 12.03s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 99/100: Loss=1.5197, F1 Score on Dev Set: 0.8113\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [20:39<00:00, 12.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 100/100: Loss=1.4929, F1 Score on Dev Set: 0.8133\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Simple_BiLSTM(embedding_dim=100, hidden_dim=256, output_dim=10, dropout=0.33).to(device)\n",
        "model.to(device)\n",
        "model.load_state_dict(torch.load('blstm1.pt'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lV5lYu7eOiOP",
        "outputId": "b7f67390-5766-4a0c-d4eb-8a7bf701aacf"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate on dev data\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    all_preds = []\n",
        "    all_true = []\n",
        "    for words, tags in dev_dataloader:\n",
        "        words, tags = words.to(device), tags.to(device)\n",
        "        output = model(words)\n",
        "\n",
        "        _, preds = torch.max(output, 2)\n",
        "\n",
        "        mask = tags != tag2idx[PAD]\n",
        "        preds = preds[mask].cpu().numpy()\n",
        "        tags = tags[mask].cpu().numpy()\n",
        "\n",
        "        all_preds.extend(preds)\n",
        "        all_true.extend(tags)\n",
        "\n",
        "    idx_to_tag = {v: k for k,v in tag2idx.items()}\n",
        "    predicted_tags = [idx_to_tag[pred] for pred in all_preds]\n",
        "\n",
        "    with open(dev_split, \"r\") as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    output_lines = []\n",
        "    pred_idx = 0\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "\n",
        "        if not line:\n",
        "            output_lines.append(\"\\n\")\n",
        "            continue\n",
        "\n",
        "        tokens = line.split()\n",
        "        tokens = tokens[:2]\n",
        "        tokens.append(predicted_tags[pred_idx].upper())\n",
        "        pred_idx += 1\n",
        "\n",
        "        new_line =  \" \".join(tokens)\n",
        "        output_lines.append(new_line + \"\\n\")\n",
        "\n",
        "with open(\"dev1.out\", \"w+\") as f:\n",
        "    f.writelines(output_lines)\n",
        "\n",
        "print(\"dev1.out GENERATED\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SoMMOtadPvhL",
        "outputId": "e9b8acf4-68ac-4a6c-fcf1-f10cf321906d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dev1.out GENERATED\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install -y perl\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wpbjspg0RZ0A",
        "outputId": "15754a40-88dc-4c6e-ba93-ce032d5d1260"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libperl5.34 perl-base perl-modules-5.34\n",
            "Suggested packages:\n",
            "  perl-doc libterm-readline-gnu-perl | libterm-readline-perl-perl libtap-harness-archive-perl\n",
            "Recommended packages:\n",
            "  netbase\n",
            "The following packages will be upgraded:\n",
            "  libperl5.34 perl perl-base perl-modules-5.34\n",
            "4 upgraded, 0 newly installed, 0 to remove and 31 not upgraded.\n",
            "Need to get 9,790 kB of archives.\n",
            "After this operation, 8,192 B of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libperl5.34 amd64 5.34.0-3ubuntu1.3 [4,820 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 perl amd64 5.34.0-3ubuntu1.3 [232 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 perl-base amd64 5.34.0-3ubuntu1.3 [1,762 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 perl-modules-5.34 all 5.34.0-3ubuntu1.3 [2,976 kB]\n",
            "Fetched 9,790 kB in 1s (9,420 kB/s)\n",
            "(Reading database ... 121749 files and directories currently installed.)\n",
            "Preparing to unpack .../libperl5.34_5.34.0-3ubuntu1.3_amd64.deb ...\n",
            "Unpacking libperl5.34:amd64 (5.34.0-3ubuntu1.3) over (5.34.0-3ubuntu1.2) ...\n",
            "Preparing to unpack .../perl_5.34.0-3ubuntu1.3_amd64.deb ...\n",
            "Unpacking perl (5.34.0-3ubuntu1.3) over (5.34.0-3ubuntu1.2) ...\n",
            "Preparing to unpack .../perl-base_5.34.0-3ubuntu1.3_amd64.deb ...\n",
            "Unpacking perl-base (5.34.0-3ubuntu1.3) over (5.34.0-3ubuntu1.2) ...\n",
            "Setting up perl-base (5.34.0-3ubuntu1.3) ...\n",
            "(Reading database ... 121749 files and directories currently installed.)\n",
            "Preparing to unpack .../perl-modules-5.34_5.34.0-3ubuntu1.3_all.deb ...\n",
            "Unpacking perl-modules-5.34 (5.34.0-3ubuntu1.3) over (5.34.0-3ubuntu1.2) ...\n",
            "Setting up perl-modules-5.34 (5.34.0-3ubuntu1.3) ...\n",
            "Setting up libperl5.34:amd64 (5.34.0-3ubuntu1.3) ...\n",
            "Setting up perl (5.34.0-3ubuntu1.3) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python '/content/eval.py' -p '/content/dev1.out' -g '/content/drive/MyDrive/nlp_hw4(dataset)/dev'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eq6bEcqPScpl",
        "outputId": "7747583b-1540-4fd3-c45f-616a543277a4"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "processed 51578 tokens with 5942 phrases; found: 5600 phrases; correct: 4402.\n",
            "accuracy:  95.40%; precision:  78.61%; recall:  74.08%; FB1:  76.28\n",
            "              LOC: precision:  88.48%; recall:  80.68%; FB1:  84.40  1675\n",
            "             MISC: precision:  81.01%; recall:  74.51%; FB1:  77.63  848\n",
            "              ORG: precision:  68.01%; recall:  68.16%; FB1:  68.08  1344\n",
            "              PER: precision:  76.11%; recall:  71.61%; FB1:  73.79  1733\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generating output files dev1.out and test1.out"
      ],
      "metadata": {
        "id": "VPp9FuAOhGg1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate on dev data\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    all_preds = []\n",
        "    all_true = []\n",
        "    for words, tags in dev_dataloader:\n",
        "        words, tags = words.to(device), tags.to(device)\n",
        "        output = model(words)\n",
        "\n",
        "        _, preds = torch.max(output, 2)\n",
        "\n",
        "        mask = tags != tag2idx[PAD]\n",
        "        preds = preds[mask].cpu().numpy()\n",
        "        tags = tags[mask].cpu().numpy()\n",
        "\n",
        "        all_preds.extend(preds)\n",
        "        all_true.extend(tags)\n",
        "\n",
        "    idx_to_tag = {v: k for k,v in tag2idx.items()}\n",
        "    predicted_tags = [idx_to_tag[pred] for pred in all_preds]\n",
        "\n",
        "    with open(dev_split, \"r\") as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    output_lines = []\n",
        "    pred_idx = 0\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "\n",
        "        if not line:\n",
        "            output_lines.append(\"\\n\")\n",
        "            continue\n",
        "\n",
        "        tokens = line.split()\n",
        "        tokens = tokens[:2]\n",
        "        tokens.append(predicted_tags[pred_idx].upper())\n",
        "        pred_idx += 1\n",
        "\n",
        "        new_line =  \" \".join(tokens)\n",
        "        output_lines.append(new_line + \"\\n\")\n",
        "\n",
        "with open(\"dev1.out\", \"w+\") as f:\n",
        "    f.writelines(output_lines)\n",
        "\n",
        "print(\"dev1.out\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWv6LRZsSugA",
        "outputId": "98bdbf9b-a380-4b94-8ac0-59dbd7f66bd8"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dev1.out\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDatasetTest(Dataset):\n",
        "    def __init__(self, x):\n",
        "        self.x = x\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.tensor(self.x[idx], dtype=torch.long)\n",
        "\n",
        "def collate_fn_test(batch):\n",
        "    padded_word_indices = pad_sequence([b for b in batch], batch_first=True, padding_value=0)\n",
        "    return padded_word_indices\n",
        "\n",
        "def convertFileToTensor_test(file_path, word2idx, tag2idx, unk_threshold=0):\n",
        "    with open(file_path, 'r') as f:\n",
        "        all_text = f.read()\n",
        "\n",
        "    sentences = all_text.strip().split('\\n\\n')\n",
        "    sentences = [s.strip().split('\\n') for s in sentences]\n",
        "    sentences = [[l.split() for l in s] for s in sentences]\n",
        "\n",
        "    words = [[w[1].lower() for w in s] for s in sentences]\n",
        "    word_freqs = Counter([w for sen in words for w in sen])\n",
        "\n",
        "\n",
        "    words = [[w if word_freqs[w] > unk_threshold else UNK for w in sen] for sen in words]\n",
        "\n",
        "    sentences_word_indices = [[word2idx.get(w, word2idx[UNK]) for w in sen] for sen in words]\n",
        "\n",
        "    return sentences_word_indices\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WeUj880DhSN2"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_padded_word_indices = convertFileToTensor_test(test_split, word2idx, tag2idx)\n",
        "test_dataset = CustomDatasetTest(test_padded_word_indices)\n",
        "test_dataloader = DataLoader(test_dataset, collate_fn = collate_fn_test, batch_size = BATCH_SIZE)"
      ],
      "metadata": {
        "id": "tX3z0WNjj4jK"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "neXUsKmyj-G7",
        "outputId": "323066b4-0711-4a36-ad40-d57360258ea3"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1249, 1249])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate on test data\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    all_preds = []\n",
        "    all_true = []\n",
        "    for words in test_dataloader:\n",
        "        words = words.to(device)\n",
        "\n",
        "        output = model(words)\n",
        "\n",
        "        _, preds = torch.max(output, 2)\n",
        "\n",
        "        for p in preds:\n",
        "            all_preds.append(p.tolist())\n",
        "\n",
        "all_true = []\n",
        "\n",
        "with open(test_split) as f:\n",
        "    all_str = f.read()\n",
        "\n",
        "sentences = all_str.split(\"\\n\\n\")\n",
        "sentences = [[i for i in sen.split(\"\\n\") if i] for sen in sentences]\n",
        "all_true = [[line.split()[1] for line in sen] for sen in sentences]\n",
        "\n",
        "idx_to_tag = {v: k for k,v in tag2idx.items()}\n",
        "\n",
        "with open(\"test1.out\", \"w+\") as f:\n",
        "    for i in range(len(all_true)):\n",
        "        T = all_true[i]\n",
        "        P = all_preds[i][:len(T)]\n",
        "\n",
        "        for j in range(len(T)):\n",
        "            f.write(f\"{j+1} {T[j]} {idx_to_tag[P[j]]}\\n\")\n",
        "        f.write(\"\\n\")\n",
        "\n",
        "with open('test1.out', 'r') as file:\n",
        "    lines = file.readlines()\n",
        "lines.pop()\n",
        "\n",
        "with open('test1.out', 'w+') as file:\n",
        "    file.writelines(lines)\n",
        "\n",
        "print(\"test1.out\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LWbZF0nSjQa8",
        "outputId": "94121094-e20f-424b-e585-dab5ec15cb94"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test1.out\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TASK 2 Using GloVe word embeddings"
      ],
      "metadata": {
        "id": "Sk7oGoOWjmkz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')\n",
        "BATCH_SIZE = 64"
      ],
      "metadata": {
        "id": "1h3W6xpKjVgy"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Glove_BiLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, num_classes, glove_vectors, embedding_dim, hidden_dim, output_dim, num_layers, dropout_rate):\n",
        "        super(Glove_BiLSTM, self).__init__()\n",
        "        self.cap_embedding = nn.Embedding(3, embedding_dim)\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.embedding.weight.data.copy_(glove_vectors)\n",
        "        self.embedding.weight.requires_grad = False\n",
        "        self.blstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True, bidirectional=True)\n",
        "        self.linear = nn.Linear(2*hidden_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.elu = nn.ELU()\n",
        "        self.classifier = nn.Linear(output_dim, num_classes)\n",
        "\n",
        "    def forward(self, sentence, x_cap):\n",
        "        sentence_embedd = self.embedding(sentence)\n",
        "        cap_embedd = self.cap_embedding(x_cap)\n",
        "        output, _ = self.blstm(sentence_embedd + cap_embedd)\n",
        "        output = self.linear(output)\n",
        "        output = self.dropout(output)\n",
        "        output = self.elu(output)\n",
        "        output = self.classifier(output)\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "OPWI0dcXkNWh"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convertFileToTensor(file_path, word2idx, tag2idx, unk_threshold=0):\n",
        "    with open(file_path, 'r') as f:\n",
        "        all_text = f.read()\n",
        "\n",
        "    sentences = all_text.strip().split('\\n\\n')\n",
        "    sentences = [s.strip().split('\\n') for s in sentences]\n",
        "    sentences = [[l.split() for l in s] for s in sentences]\n",
        "\n",
        "    words = [[w[1].lower() for w in s] for s in sentences]\n",
        "    word_freqs = Counter([w for sen in words for w in sen])\n",
        "    words_set = [w for w, freq in word_freqs.items() if freq > unk_threshold]\n",
        "\n",
        "    words = [[w if word_freqs[w] > unk_threshold else UNK for w in sen] for sen in words]\n",
        "    ners = [[l[2] for l in s] for s in sentences]\n",
        "\n",
        "    sentences_word_indices = [[word2idx.get(w, word2idx[UNK]) for w in sen] for sen in words]\n",
        "    sentences_cap_indices = [[word2cap(w[1]) for w in s] for s in sentences]\n",
        "    sentences_tag_indices = [[tag2idx[t] for t in sen] for sen in ners]\n",
        "\n",
        "    return sentences_word_indices, sentences_cap_indices, sentences_tag_indices"
      ],
      "metadata": {
        "id": "XDWlGXfrkkB0"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "  padded_word_indices = pad_sequence([b[0] for b in batch], batch_first=True,padding_value=0)\n",
        "  padded_cap_indices = pad_sequence([b[1] for b in batch], batch_first=True, padding_value=0)\n",
        "  padded_tag_indices = pad_sequence([b[2] for b in batch], batch_first=True, padding_value=0)\n",
        "  return padded_word_indices, padded_cap_indices, padded_tag_indices"
      ],
      "metadata": {
        "id": "PTfLO0xCkn7o"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, x, y, z):\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "        self.z = z\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.tensor(self.x[idx], dtype=torch.long), \\\n",
        "                torch.tensor(self.y[idx], dtype=torch.long), \\\n",
        "                torch.tensor(self.z[idx], dtype=torch.long)"
      ],
      "metadata": {
        "id": "QV_x_Nb6FAgd"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# Set the random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "\n",
        "unk_threshold = 1\n",
        "\n",
        "embedding_dim = 100\n",
        "embedding_path = '/content/drive/MyDrive/nlp_hw4(dataset)/glove.6B.100d.txt'\n",
        "\n",
        "word_vectors = {}\n",
        "pad_vector = np.zeros(100)\n",
        "unk_vector = np.random.randn(100)\n",
        "word_vectors['<PAD>'] = pad_vector\n",
        "word_vectors['<UNK>'] = unk_vector\n",
        "\n",
        "with open(embedding_path, 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        word, *vector = line.split()\n",
        "        vector = list(map(float, vector))\n",
        "        word_vectors[word] = np.array(vector)\n",
        "\n",
        "\n",
        "glove_vectors = torch.tensor(list(word_vectors.values()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fnQoioMskukw",
        "outputId": "e9c9bc5d-633f-4dc2-aacd-46f48adaf2e8"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-33-c7899a8d616c>:23: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:261.)\n",
            "  glove_vectors = torch.tensor(list(word_vectors.values()))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.vocab import GloVe\n",
        "glove = GloVe(name=\"6B\", dim=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w_0cm88HlwP4",
        "outputId": "b93123f1-bd5a-44da-a179-e3d95ac46b38"
      },
      "execution_count": 34,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [02:40, 5.36MB/s]                           \n",
            "100%|█████████▉| 399999/400000 [00:20<00:00, 19123.01it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pad_vector = torch.zeros(1, glove.dim)\n",
        "unk_vector = torch.randn(1, glove.dim)\n",
        "glove_vectors = torch.cat([pad_vector, unk_vector, glove.vectors], dim=0)\n",
        "glove.itos.insert(0, '<PAD>')\n",
        "glove.itos.insert(1, '<UNK>')"
      ],
      "metadata": {
        "id": "4e2m9G4Kl4JC"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word2idx = {word: idx for idx, word in enumerate(glove.itos)}\n",
        "idx2word = {idx: word for idx, word in enumerate(glove.itos)}\n",
        "tag2idx = {'<PAD>': 0,'B-PER': 4, 'I-MISC': 6, 'O': 9, 'B-LOC': 1, 'I-ORG': 7,'I-LOC': 5, 'B-ORG': 3, 'B-MISC': 2, 'I-PER': 8}\n",
        "vocab_size, num_classes = len(word2idx), len(tag2idx)"
      ],
      "metadata": {
        "id": "kmrODtXGl7n0"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def word2cap(x):\n",
        "  return 1 if x == x.lower() else 2"
      ],
      "metadata": {
        "id": "uULM5QdonR1e"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_padded_word_indices, train_padded_cap_indices, train_padded_tag_indices = convertFileToTensor(train_split, word2idx, tag2idx)\n",
        "train_dataset = CustomDataset(train_padded_word_indices, train_padded_cap_indices, train_padded_tag_indices)\n",
        "train_dataloader = DataLoader(train_dataset, collate_fn=collate_fn, batch_size = BATCH_SIZE)\n"
      ],
      "metadata": {
        "id": "4GjZytYInUe2"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dev_padded_word_indices, dev_padded_cap_indices, dev_padded_tag_indices = convertFileToTensor(dev_split, word2idx, tag2idx)\n",
        "dev_dataset = CustomDataset(dev_padded_word_indices, dev_padded_cap_indices, dev_padded_tag_indices)\n",
        "dev_dataloader = DataLoader(dev_dataset, collate_fn=collate_fn, batch_size = BATCH_SIZE)"
      ],
      "metadata": {
        "id": "P_ebbbmnwuJx"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.cuda.device_count())\n",
        "print(torch.cuda.get_device_name(0))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fsAE2RIZoEGo",
        "outputId": "34c52e97-bc21-460e-fca3-c0c7685bd6f2"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "Tesla V100-SXM2-16GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "metadata": {
        "id": "eDgmgyyUGuqs"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Glove_BiLSTM(vocab_size, num_classes, glove_vectors, embedding_dim=100, hidden_dim=256, output_dim=128, num_layers=1, dropout_rate=0.33)\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nA8x1YrDngQp",
        "outputId": "f0c89ba4-bacf-463a-9def-8b5f3345592b"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Glove_BiLSTM(\n",
              "  (cap_embedding): Embedding(3, 100)\n",
              "  (embedding): Embedding(400002, 100)\n",
              "  (blstm): LSTM(100, 256, batch_first=True, bidirectional=True)\n",
              "  (linear): Linear(in_features=512, out_features=128, bias=True)\n",
              "  (dropout): Dropout(p=0.33, inplace=False)\n",
              "  (elu): ELU(alpha=1.0)\n",
              "  (classifier): Linear(in_features=128, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss(ignore_index=tag2idx[PAD])\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.5)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.9)"
      ],
      "metadata": {
        "id": "yUAEPOQnnnLF"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 50"
      ],
      "metadata": {
        "id": "cHSmbbm_oTvA"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "LOSS = 10000000\n",
        "for epoch in tqdm(range(num_epochs)):\n",
        "  model.train()\n",
        "  epoch_loss = 0\n",
        "  for words, caps, tags in train_dataloader:\n",
        "    words, caps, tags = words.to(device), caps.to(device), tags.to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    output = model(words, caps)\n",
        "    output = output.view(-1, num_classes)\n",
        "    loss = criterion(output, tags.view(-1))\n",
        "    epoch_loss = loss / len(train_dataloader)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "  if epoch_loss < LOSS:\n",
        "    LOSS = epoch_loss\n",
        "    torch.save(model.state_dict(), 'blstm2.pt')\n",
        "# Print epoch metrics\n",
        "  if (epoch+1) % 10 == 0:\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}: Loss={epoch_loss:.8f}')\n",
        "# scheduler.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ruRejXOoWfO",
        "outputId": "a9ca947a-bced-46c4-be99-fa3f6c1092d5"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 20%|██        | 10/50 [00:21<01:19,  1.99s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/50: Loss=0.00003911\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 40%|████      | 20/50 [00:41<01:00,  2.00s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20/50: Loss=0.00001906\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 60%|██████    | 30/50 [00:59<00:34,  1.71s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 30/50: Loss=0.00002208\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 80%|████████  | 40/50 [01:17<00:18,  1.86s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 40/50: Loss=0.00000485\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 50/50 [01:35<00:00,  1.91s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 50/50: Loss=0.00000280\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Glove_BiLSTM(vocab_size, num_classes, glove_vectors, embedding_dim=100, hidden_dim=256, output_dim=128, num_layers=1, dropout_rate=0.33)\n",
        "model.to(device)\n",
        "model.load_state_dict(torch.load('blstm2.pt'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55fgl8MVow3g",
        "outputId": "3d70e150-6ddb-4aa2-d480-498c64254cf1"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate on dev data\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    all_preds = []\n",
        "    all_true = []\n",
        "    for words, caps, tags in dev_dataloader:\n",
        "        words, caps, tags = words.to(device), caps.to(device), tags.to(device)\n",
        "        output = model(words, caps)\n",
        "\n",
        "        _, preds = torch.max(output, 2)\n",
        "\n",
        "        mask = tags != tag2idx[PAD]\n",
        "        preds = preds[mask].cpu().numpy()\n",
        "        tags = tags[mask].cpu().numpy()\n",
        "\n",
        "        all_preds.extend(preds)\n",
        "        all_true.extend(tags)\n",
        "\n",
        "    idx_to_tag = {v: k for k,v in tag2idx.items()}\n",
        "    predicted_tags = [idx_to_tag[pred] for pred in all_preds]\n",
        "\n",
        "    with open(dev_split, \"r\") as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    output_lines = []\n",
        "    pred_idx = 0\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "\n",
        "        if not line:\n",
        "            output_lines.append(\"\\n\")\n",
        "            continue\n",
        "\n",
        "        tokens = line.split()\n",
        "        tokens = tokens[:2]\n",
        "        tokens.append(predicted_tags[pred_idx].upper())\n",
        "        pred_idx += 1\n",
        "\n",
        "        new_line =  \" \".join(tokens)\n",
        "        output_lines.append(new_line + \"\\n\")\n",
        "\n",
        "with open(\"dev2.out\", \"w+\") as f:\n",
        "    f.writelines(output_lines)\n",
        "\n",
        "print(\"dev2.out\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96-y9_ojo97Q",
        "outputId": "f1f95c32-7ea4-4062-881d-88fb4130c36d"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dev2.out\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python '/content/eval.py' -p '/content/dev2.out' -g '/content/drive/MyDrive/nlp_hw4(dataset)/dev'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w3F8e1mcpMgn",
        "outputId": "327f52f9-97b6-43d9-c842-e44a611a3537"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "processed 51578 tokens with 5942 phrases; found: 6106 phrases; correct: 5459.\n",
            "accuracy:  98.50%; precision:  89.40%; recall:  91.87%; FB1:  90.62\n",
            "              LOC: precision:  94.56%; recall:  94.67%; FB1:  94.61  1839\n",
            "             MISC: precision:  79.36%; recall:  85.47%; FB1:  82.30  993\n",
            "              ORG: precision:  83.99%; recall:  86.43%; FB1:  85.19  1380\n",
            "              PER: precision:  93.61%; recall:  96.25%; FB1:  94.91  1894\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDatasetTest(Dataset):\n",
        "    def __init__(self, x, y):\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.tensor(self.x[idx], dtype=torch.long), \\\n",
        "                torch.tensor(self.y[idx], dtype=torch.long)\n",
        "\n",
        "def collate_fn_test(batch):\n",
        "    padded_word_indices = pad_sequence([b[0] for b in batch], batch_first=True, padding_value=0)\n",
        "    padded_cap_indices = pad_sequence([b[1] for b in batch], batch_first=True, padding_value=0)\n",
        "    return padded_word_indices, padded_cap_indices\n",
        "\n",
        "def convertFileToTensor_test(file_path, word2idx, tag2idx, unk_threshold=0):\n",
        "    with open(file_path, 'r') as f:\n",
        "        all_text = f.read()\n",
        "\n",
        "    sentences = all_text.strip().split('\\n\\n')\n",
        "    sentences = [s.strip().split('\\n') for s in sentences]\n",
        "    sentences = [[l.split() for l in s] for s in sentences]\n",
        "\n",
        "    words = [[w[1].lower() for w in s] for s in sentences]\n",
        "    word_freqs = Counter([w for sen in words for w in sen])\n",
        "    words_set = [w for w, freq in word_freqs.items() if freq > unk_threshold]\n",
        "\n",
        "    words = [[w if word_freqs[w] > unk_threshold else UNK for w in sen] for sen in words]\n",
        "\n",
        "    sentences_word_indices = [[word2idx.get(w, word2idx[UNK]) for w in sen] for sen in words]\n",
        "    sentences_cap_indices = [[word2cap(w[1]) for w in s] for s in sentences]\n",
        "\n",
        "\n",
        "    return sentences_word_indices, sentences_cap_indices\n",
        "\n",
        "test_padded_word_indices, test_padded_cap_indices = convertFileToTensor_test(test_split, word2idx, tag2idx)\n",
        "\n",
        "test_dataset = CustomDatasetTest(test_padded_word_indices, test_padded_cap_indices)\n",
        "test_dataloader = DataLoader(test_dataset, collate_fn=collate_fn_test, batch_size = BATCH_SIZE)"
      ],
      "metadata": {
        "id": "fc163xXRpViL"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate on dev data\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    all_preds = []\n",
        "\n",
        "    for words, caps in test_dataloader:\n",
        "        words, caps = words.to(device), caps.to(device)\n",
        "\n",
        "        output = model(words, caps)\n",
        "\n",
        "        _, preds = torch.max(output, 2)\n",
        "\n",
        "        for p in preds:\n",
        "            all_preds.append(p.tolist())\n",
        "all_true = []\n",
        "\n",
        "with open(test_split) as f:\n",
        "    all_str = f.read()\n",
        "\n",
        "sentences = all_str.split(\"\\n\\n\")\n",
        "sentences = [[i for i in sen.split(\"\\n\") if i] for sen in sentences]\n",
        "all_true = [[line.split()[1] for line in sen] for sen in sentences]\n",
        "\n",
        "\n",
        "idx_to_tag = {v: k for k,v in tag2idx.items()}\n"
      ],
      "metadata": {
        "id": "fcyhiTGgptEW"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"test2.out\", \"w+\") as f:\n",
        "    for i in range(len(all_true)):\n",
        "        T = all_true[i]\n",
        "        P = all_preds[i][:len(T)]\n",
        "\n",
        "        for j in range(len(T)):\n",
        "            f.write(f\"{j+1} {T[j]} {idx_to_tag[P[j]]}\\n\")\n",
        "        f.write(\"\\n\")\n",
        "\n",
        "with open('test2.out', 'r') as file:\n",
        "    lines = file.readlines()\n",
        "lines.pop()\n",
        "\n",
        "with open('test2.out', 'w+') as file:\n",
        "    file.writelines(lines)\n",
        "\n",
        "print(\"test2.out\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VgNqMmMQpyoj",
        "outputId": "cbc99013-f860-4295-e1cd-03895da3ea8d"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test2.out\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 3 - CNN - BiLSTM"
      ],
      "metadata": {
        "id": "n5sE37W5p7hk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "np.random.seed(42)\n",
        "import os\n",
        "import gzip\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau\n",
        "\n"
      ],
      "metadata": {
        "id": "edOV0uNPiUHw"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN_FILE = train_split\n",
        "DEV_FILE = dev_split\n",
        "TEST_FILE = test_split"
      ],
      "metadata": {
        "id": "_zUqQV4vS1dF"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k9AWgc9ay6Pz",
        "outputId": "592adc9a-13bc-4c23-8b02-572d6985fbb7"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LEN = 120"
      ],
      "metadata": {
        "id": "ZIUHxM2lS4c1"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming TRAIN_FILE, DEV_FILE, and TEST_FILE are defined file paths\n",
        "# Read the training data\n",
        "with open(TRAIN_FILE, \"r\") as f:\n",
        "    train = f.readlines()\n",
        "\n",
        "# Add an empty string at the end of the list to signify the end of the file\n",
        "train += [\"\"]\n",
        "\n",
        "# Strip the newline characters from the end of each line\n",
        "train = [i[:-1] for i in train]\n",
        "# Initialize lists to store the processed training data\n",
        "train_x, train_y = [], []\n",
        "\n",
        "# Read the development data\n",
        "with open(DEV_FILE, \"r\") as f:\n",
        "    dev = f.readlines()\n",
        "\n",
        "# Add an empty string at the end of the list to signify the end of the file\n",
        "dev += [\"\"]\n",
        "\n",
        "# Strip the newline characters from the end of each line\n",
        "dev = [i[:-1] for i in dev]\n",
        "# Initialize lists to store the processed development data\n",
        "dev_x, dev_y = [], []\n",
        "\n",
        "# Read the test data\n",
        "with open(TEST_FILE, \"r\") as f:\n",
        "    test = f.readlines()\n",
        "\n",
        "# Add an empty string at the end of the list to signify the end of the file\n",
        "test += [\"\"]\n",
        "\n",
        "# Strip the newline characters from the end of each line\n",
        "test = [i[:-1] for i in test]\n",
        "# Initialize lists to store the processed test data\n",
        "test_x, test_y = [], []\n"
      ],
      "metadata": {
        "id": "nbEehJhmTCyd"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "# Initialize empty lists for sentences and labels\n",
        "sent = []\n",
        "label = []\n",
        "\n",
        "# Initialize the variable to hold the maximum sentence length\n",
        "MAX_SENT_LEN = 0\n",
        "\n",
        "# Initialize a set to keep track of all unique tags\n",
        "all_unique_tags = set()\n",
        "\n",
        "# Loop through each line in the training data\n",
        "for x in tqdm(train):\n",
        "    # Split the line into its components\n",
        "    k = x.split(\" \")\n",
        "    # If the line is empty (i.e., it's the end of a sentence)\n",
        "    if len(k) == 1:\n",
        "        # Update the maximum sentence length\n",
        "        MAX_SENT_LEN = max(MAX_SENT_LEN, len(sent))\n",
        "        # Pad the sentence and label lists to the maximum length\n",
        "        while len(sent) < MAX_LEN and len(label) < MAX_LEN:\n",
        "            sent.append(\"<pad>\")\n",
        "            label.append(\"<pad>\")\n",
        "        # Append the sentence and label lists to the training data\n",
        "        train_x.append(sent[:MAX_LEN])\n",
        "        train_y.append(label[:MAX_LEN])\n",
        "        # Reset the sentence and label lists\n",
        "        sent = []\n",
        "        label = []\n",
        "        continue\n",
        "    # If the line is not empty, add the word and its label to the lists\n",
        "    sent.append(k[1])\n",
        "    label.append(k[2])\n",
        "    # Add the label to the set of unique tags\n",
        "    all_unique_tags.add(k[2])\n",
        "\n",
        "# Print the maximum sentence length\n",
        "print(\"MAX_SENT_LEN\", MAX_SENT_LEN)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jy1W20VyTMjP",
        "outputId": "3e5b2de8-c7ef-4574-bd95-1b6f973258ba"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 219554/219554 [00:01<00:00, 200152.93it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAX_SENT_LEN 113\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_embeddings(path):\n",
        "  embeddings_index = {}\n",
        "  with open(path) as f:\n",
        "    for line in f:\n",
        "      values = line.split()\n",
        "      word = values[0]\n",
        "      coefs = np.asarray(values[1:], dtype='float32')\n",
        "      embeddings_index[word] = coefs\n",
        "  return embeddings_index\n",
        "\n",
        "embeddings_index = load_embeddings('/content/drive/MyDrive/nlp_hw4(dataset)/glove.6B.100d.txt')"
      ],
      "metadata": {
        "id": "IlG7fy5BTQNu"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "padding_vector = np.random.uniform(low=-1, high=1, size=(100,))\n",
        "embeddings_index[\"<pad>\"] = padding_vector\n",
        "all_unique_tags.add(\"<pad>\")"
      ],
      "metadata": {
        "id": "Jn8yL8XFTgSq"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "char2idx = {}\n",
        "char2idx['<pad>'] = 0\n",
        "char2idx['<unk>'] = 1\n",
        "idx = 2\n",
        "for word in list(embeddings_index.keys()):\n",
        "  for char in word:\n",
        "    x = char.lower()\n",
        "    if x not in char2idx:\n",
        "      char2idx[x] = idx\n",
        "      idx += 1\n",
        "    y = char.upper()\n",
        "    if y not in char2idx:\n",
        "      char2idx[y] = idx\n",
        "      idx += 1\n",
        "idx2char = {v:k for k, v in char2idx.items()}\n",
        "\n",
        "word2idx = {}\n",
        "idx2word = {}\n",
        "for e, w in enumerate(embeddings_index.keys()):\n",
        "  word2idx[w] = e\n",
        "  idx2word[e] = w\n",
        "tag2idx = {}\n",
        "idx2tag = {}\n",
        "for e, k in enumerate(list(all_unique_tags)):\n",
        "  tag2idx[k] = e\n",
        "  idx2tag[e] = k"
      ],
      "metadata": {
        "id": "vnbtbz4MUq0h"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preProcessData(x, y):\n",
        "    finalX, charsX, finalY = [], [], []\n",
        "    for i in x:\n",
        "        p = []\n",
        "        for c in i:\n",
        "            p.append(char2idx[c])\n",
        "        p = p + [char2idx[\"<pad>\"]] * 100\n",
        "        p = p[:32]\n",
        "        charsX.append(p)\n",
        "\n",
        "        i = i.lower()\n",
        "        try:\n",
        "            finalX.append(word2idx[i])\n",
        "        except:\n",
        "            finalX.append(word2idx[\"unk\"])\n",
        "\n",
        "    for i in y:\n",
        "        finalY.append(tag2idx[i])\n",
        "\n",
        "    return finalX, charsX[:MAX_LEN], finalY\n",
        "\n",
        "preprocessed_train_x = []\n",
        "preprocessed_train_y = []\n",
        "preprocessed_chars_x = []\n",
        "\n",
        "for i in tqdm(range(len(train_x))):\n",
        "    finalX, charsX, finalY = preProcessData(train_x[i], train_y[i])\n",
        "    preprocessed_train_x.append(torch.tensor(finalX))\n",
        "    preprocessed_chars_x.append(torch.tensor(charsX))\n",
        "    preprocessed_train_y.append(torch.tensor(finalY))\n",
        "\n",
        "len(preprocessed_train_x), len(preprocessed_train_y), len(preprocessed_chars_x)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NsW8XZj7UvJi",
        "outputId": "9a5704bf-5ade-47d4-d801-da78d6ac3508"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14987/14987 [00:22<00:00, 657.96it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(14987, 14987, 14987)"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "class CustomDataset(Dataset):\n",
        "  def __init__(self, x, y, chars):\n",
        "    self.x = x\n",
        "    self.y = y\n",
        "    self.chars = chars\n",
        "  def __len__(self):\n",
        "    return len(self.x)\n",
        "  def __getitem__(self, idx):\n",
        "    return self.x[idx], self.chars[idx], self.y[idx]\n",
        "# create custom dataset with characters\n",
        "dataset = CustomDataset(preprocessed_train_x, preprocessed_train_y, preprocessed_chars_x)\n",
        "# create data loader\n",
        "batch_size = 128\n",
        "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "ldYNlHhKUxbC"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "class CharCNN(nn.Module):\n",
        "    def __init__(self, char_vocab_size, char_embedding_dim, output_dim):\n",
        "        super(CharCNN, self).__init__()\n",
        "        self.char_embedding = nn.Embedding(char_vocab_size, char_embedding_dim)\n",
        "        self.conv1d = nn.Conv1d(char_embedding_dim, output_dim, kernel_size=32)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [batch_size, max_seq_len, max_word_len]\n",
        "        x = self.char_embedding(x)  # [batch_size, max_seq_len, max_word_len, char_embedding_dim]\n",
        "        x = x.permute(0, 1, 3, 2)  # [batch_size, max_seq_len, char_embedding_dim, max_word_len]\n",
        "        batch_size, max_seq_len, char_embedding_dim, max_word_len = x.shape\n",
        "        x = x.view(-1, char_embedding_dim, max_word_len)  # [batch_size * max_seq_len, char_embedding_dim, max_word_len]\n",
        "        x = self.conv1d(x)  # [batch_size * max_seq_len, output_dim, max_word_len - kernel_size + 1]\n",
        "        x = F.relu(x)\n",
        "        x = F.max_pool1d(x, kernel_size=x.shape[2]).squeeze()  # [batch_size * max_seq_len, output_dim]\n",
        "        x = x.view(batch_size, max_seq_len, -1)  # [batch_size, max_seq_len, output_dim]\n",
        "        return x"
      ],
      "metadata": {
        "id": "5QdSrxD6U6gK"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class cNNBiLSTM(nn.Module):\n",
        "    def __init__(self, embedding_dim, hidden_dim, output_dim, dropout, char_embedding_dim=30):\n",
        "        super(cNNBiLSTM, self).__init__()\n",
        "        # Word-level embeddings\n",
        "        self.word_embedding = nn.Embedding(len(embeddings_index.keys()), 100, padding_idx=0)\n",
        "\n",
        "        # LSTM layer\n",
        "        self.lstm = nn.LSTM(100 + output_dim, hidden_dim, bidirectional=True)\n",
        "\n",
        "        # Fully connected layer\n",
        "        self.fc = nn.Linear(hidden_dim*2, output_dim)\n",
        "\n",
        "        # Dropout layer\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.char_cnn = CharCNN(len(char2idx), 30, output_dim)\n",
        "\n",
        "    def forward(self, sentence, chars):\n",
        "        # Word-level embeddings\n",
        "        word_embedded = self.word_embedding(sentence)\n",
        "\n",
        "        # Character-level embeddings\n",
        "        char_embedded = self.char_cnn(chars)\n",
        "\n",
        "        # Concatenate word-level and character-level embeddings\n",
        "        combined_embedded = torch.cat((word_embedded, char_embedded), dim=2)\n",
        "\n",
        "        # LSTM layer\n",
        "        lstm_output, _ = self.lstm(combined_embedded)\n",
        "\n",
        "        # Fully connected layer\n",
        "        fc_output = self.fc(self.dropout(lstm_output))\n",
        "\n",
        "        # Return output\n",
        "        return fc_output\n"
      ],
      "metadata": {
        "id": "J_BJ5uftU9EM"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model3 = cNNBiLSTM(embedding_dim=100, hidden_dim=256, output_dim=len(all_unique_tags), dropout=0.33, char_embedding_dim=30).to(device)"
      ],
      "metadata": {
        "id": "hwhVoEc5VC4t"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model3.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "OE6MU4IhVJEi"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define number of epochs\n",
        "num_epochs = 200\n",
        "acc = 0\n",
        "# Train loop\n",
        "model3.train()\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "    pad_predictions = 0\n",
        "# Set model to train mode\n",
        "    for batch_idx, (inputs, chars, targets) in enumerate(train_loader):\n",
        "# Move data to device\n",
        "      inputs, targets = inputs.to(device), targets.to(device)\n",
        "      chars = chars.to(device)\n",
        "# print(inputs.shape)\n",
        "# print(chars.shape)\n",
        "\n",
        "# Clear optimizer gradients\n",
        "      optimizer.zero_grad()\n",
        "# print(inputs.shape)\n",
        "# Forward pass\n",
        "      outputs = model3(inputs, chars)\n",
        "# One-hot encode targets\n",
        "# targets_one_hot = torch.nn.functional.one_hot(targets, num_classes=len(tag2idx))\n",
        "      loss = criterion(outputs.view(-1,len(tag2idx.keys())),targets.view(-1))\n",
        "# Backward pass\n",
        "      loss.backward()\n",
        "# Update optimizer parameters\n",
        "      optimizer.step()\n",
        "# Calculate running loss\n",
        "      running_loss += loss.item()\n",
        "# Calculate accuracy\n",
        "      predicted_classes = torch.argmax(outputs, dim=2)\n",
        "# print(predicted_classes)\n",
        "      correct_predictions += torch.sum(predicted_classes == targets).item()\n",
        "      total_predictions += targets.size(0) * targets.size(1)\n",
        "      pad_predictions += torch.sum(targets == tag2idx[\"<pad>\"]).item()\n",
        "\n",
        "# scheduler.step()\n",
        "# Calculate epoch loss and accuracy\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    epoch_acc = (correct_predictions - pad_predictions) / (total_predictions - pad_predictions)\n",
        "\n",
        "    if epoch_acc > acc:\n",
        "      acc = epoch_acc\n",
        "      torch.save(model3.state_dict(), 'blstm3.pt')\n",
        "# Print epoch metrics\n",
        "    if (epoch+1) % 10 == 0:\n",
        "      print(f'Epoch {epoch+1}/{num_epochs}: Loss={epoch_loss:.4f}, Acc={epoch_acc:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ej9zewJfVLqy",
        "outputId": "1567ab57-b959-472c-93ff-7bcbf4dae91d"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/200: Loss=0.0189, Acc=0.9481\n",
            "Epoch 20/200: Loss=0.0113, Acc=0.9682\n",
            "Epoch 30/200: Loss=0.0093, Acc=0.9725\n",
            "Epoch 40/200: Loss=0.0083, Acc=0.9745\n",
            "Epoch 50/200: Loss=0.0078, Acc=0.9759\n",
            "Epoch 60/200: Loss=0.0074, Acc=0.9762\n",
            "Epoch 70/200: Loss=0.0069, Acc=0.9779\n",
            "Epoch 80/200: Loss=0.0065, Acc=0.9789\n",
            "Epoch 90/200: Loss=0.0064, Acc=0.9797\n",
            "Epoch 100/200: Loss=0.0061, Acc=0.9803\n",
            "Epoch 110/200: Loss=0.0059, Acc=0.9806\n",
            "Epoch 120/200: Loss=0.0059, Acc=0.9810\n",
            "Epoch 130/200: Loss=0.0057, Acc=0.9814\n",
            "Epoch 140/200: Loss=0.0056, Acc=0.9819\n",
            "Epoch 150/200: Loss=0.0055, Acc=0.9823\n",
            "Epoch 160/200: Loss=0.0054, Acc=0.9827\n",
            "Epoch 170/200: Loss=0.0053, Acc=0.9825\n",
            "Epoch 180/200: Loss=0.0052, Acc=0.9830\n",
            "Epoch 190/200: Loss=0.0051, Acc=0.9833\n",
            "Epoch 200/200: Loss=0.0050, Acc=0.9836\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model3 = cNNBiLSTM(embedding_dim=100, hidden_dim=256, output_dim=len(all_unique_tags), dropout=0.33, char_embedding_dim=30).to(device)\n",
        "model3.load_state_dict(torch.load('blstm3.pt'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yKkfV1vvVSzM",
        "outputId": "d3fffac4-4f25-4bec-b01c-b75d02bf81fe"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dev_sent = []\n",
        "temp = []\n",
        "for k in dev:\n",
        "    if k == \"\":\n",
        "        dev_sent.append(temp)\n",
        "        temp = []\n",
        "        continue\n",
        "    temp.append(k)\n",
        "\n",
        "dev_x, dev_y = [], []\n",
        "sent = []\n",
        "label = []\n",
        "\n",
        "for x in tqdm(dev):\n",
        "    k = x.split(\" \")\n",
        "    if len(k) == 1:\n",
        "        while len(sent) < MAX_LEN and len(label) < MAX_LEN:\n",
        "            sent.append(\"<pad>\")\n",
        "            label.append(\"<pad>\")\n",
        "        dev_x.append(sent[:MAX_LEN])\n",
        "        dev_y.append(label[:MAX_LEN])\n",
        "        sent = []\n",
        "        label = []\n",
        "        continue\n",
        "    sent.append(k[1])\n",
        "    label.append(k[2])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AeumkEs-VWMe",
        "outputId": "9b98bef1-6574-4a14-854c-948839fa644a"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 55044/55044 [00:00<00:00, 256738.72it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "preprocessed_dev_x = []\n",
        "preprocessed_dev_y = []\n",
        "preprocessed_chars_x = []\n",
        "\n",
        "for i in tqdm(range(len(dev_x))):\n",
        "    finalx, charsX, finaly = preProcessData(dev_x[i], dev_y[i])\n",
        "    preprocessed_dev_x.append(torch.tensor(finalx))\n",
        "    preprocessed_dev_y.append(torch.tensor(finaly))\n",
        "    preprocessed_chars_x.append(torch.tensor(charsX))\n",
        "\n",
        "print(len(preprocessed_dev_x), len(preprocessed_dev_y), len(preprocessed_chars_x))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0cDSGBqQVZaf",
        "outputId": "10cfcccf-2911-484c-d82b-42e4991a3e5e"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3466/3466 [00:03<00:00, 941.83it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3466 3466 3466\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_dev = []\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with open(\"dev3.out\", \"w+\") as f:\n",
        "    for i in range(len(preprocessed_dev_x)):\n",
        "        inputs, targets = preprocessed_dev_x[i].to(device), preprocessed_dev_y[i].to(device)\n",
        "        chars = preprocessed_chars_x[i].to(device)\n",
        "        inputs = inputs.unsqueeze(0)\n",
        "        chars = chars.unsqueeze(0)\n",
        "        # print(chars.shape)\n",
        "        outputs = model3(inputs, chars)\n",
        "        targets_one_hot = torch.nn.functional.one_hot(targets, num_classes=10)\n",
        "        predicted_classes = torch.argmax(outputs, dim=2)\n",
        "        # print(predicted_classes)\n",
        "        tag_count = MAX_LEN\n",
        "        # print(endToken, padToken)\n",
        "        for j in range(len(targets)):\n",
        "            if targets[j].item() == word2idx[\"<pad>\"]:\n",
        "                tag_count = j\n",
        "                break\n",
        "        # print(tag_count)\n",
        "        T = targets[:tag_count]\n",
        "        P = predicted_classes[0][:tag_count].tolist()\n",
        "        original = dev_sent[i]\n",
        "        P = P + [tag2idx['O']]*10000\n",
        "        P = P[:len(original)]\n",
        "        for e, p in enumerate(original):\n",
        "            f.write(f\"{p} {idx2tag[P[e]]}\\n\")\n",
        "        f.write(f\"\\n\")\n",
        "\n",
        "# Remove the last empty line from the output file\n",
        "with open('dev3.out', 'r') as file:\n",
        "    lines = file.readlines()\n",
        "    lines.pop()\n",
        "\n",
        "with open('dev3.out', 'w+') as file:\n",
        "    file.writelines(lines)\n"
      ],
      "metadata": {
        "id": "SSNfOZMuVc5R"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model3 = cNNBiLSTM(embedding_dim=100, hidden_dim=256, output_dim=len(all_unique_tags), dropout=0.33, char_embedding_dim=30).to(device)\n",
        "model3.load_state_dict(torch.load('blstm3.pt'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKx08Q8afX1H",
        "outputId": "ea492aa5-e5d4-4eab-e1c9-df533f0358f7"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python '/content/eval.py' -p '/content/dev3.out' -g '/content/drive/MyDrive/nlp_hw4(dataset)/dev'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QuEJom28XI2B",
        "outputId": "b92a94d5-47a6-448f-c090-989033e6991f"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "processed 51578 tokens with 5942 phrases; found: 7136 phrases; correct: 4077.\n",
            "accuracy:  93.79%; precision:  57.13%; recall:  68.61%; FB1:  62.35\n",
            "              LOC: precision:  72.83%; recall:  82.14%; FB1:  77.21  2072\n",
            "             MISC: precision:  61.09%; recall:  70.50%; FB1:  65.46  1064\n",
            "              ORG: precision:  44.78%; recall:  61.74%; FB1:  51.91  1849\n",
            "              PER: precision:  50.67%; recall:  59.17%; FB1:  54.60  2151\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Test set evaluation"
      ],
      "metadata": {
        "id": "zGMQyeMwj5Bd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_sent = []\n",
        "temp = []\n",
        "\n",
        "# Loop over each line in the test dataset\n",
        "for k in test:\n",
        "    # Check if the line is empty (sentence delimiter)\n",
        "    if k == \"\":\n",
        "        test_sent.append(temp)\n",
        "        temp = []\n",
        "        continue\n",
        "    # Add the non-empty line to the temporary list\n",
        "    temp.append(k)\n",
        "\n",
        "# List to store processed sentences\n",
        "test_x = []\n",
        "# Temporary storage for the current sentence\n",
        "sent = []\n",
        "\n",
        "# Loop over the test dataset\n",
        "for x in tqdm(test):\n",
        "    # Split the line by spaces\n",
        "    k = x.split(\" \")\n",
        "    # Check if the line is empty (sentence delimiter)\n",
        "    if len(k) == 1:\n",
        "        # If the sentence is shorter than MAX_LEN, pad the sentence\n",
        "        while len(sent) < MAX_LEN:\n",
        "            sent.append(\"<pad>\")\n",
        "        # Add the padded sentence to the list of processed sentences\n",
        "        test_x.append(sent[:MAX_LEN])\n",
        "        # Reset the sentence list for the next sentence\n",
        "        sent = []\n",
        "        continue\n",
        "    # Append the word to the current sentence list\n",
        "    sent.append(k[1])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KoM4A-zrXj_P",
        "outputId": "16253216-b6a2-44d2-9710-2eedb5ee8c39"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 50350/50350 [00:00<00:00, 405544.43it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preProcessData(x):\n",
        "    finalX, charsX = [], []\n",
        "    for i in x:\n",
        "        p = []\n",
        "        for c in i:\n",
        "            p.append(char2idx[c])\n",
        "        p = p + [char2idx[\"<pad>\"]] * 100\n",
        "        p = p[:32]\n",
        "        charsX.append(p)\n",
        "\n",
        "        i = i.lower()\n",
        "        try:\n",
        "            finalX.append(word2idx[i])\n",
        "        except:\n",
        "            finalX.append(word2idx[\"unk\"])\n",
        "\n",
        "    return finalX, charsX[:MAX_LEN]\n",
        "\n",
        "preprocessed_test_x = []\n",
        "preprocessed_chars_x = []\n",
        "\n",
        "for i in tqdm(range(len(test_x))):\n",
        "    finalX, charsX = preProcessData(test_x[i])\n",
        "    preprocessed_test_x.append(torch.tensor(finalX))\n",
        "    preprocessed_chars_x.append(torch.tensor(charsX))\n",
        "\n",
        "len(preprocessed_test_x), len(preprocessed_chars_x)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQWIt59lX64y",
        "outputId": "8f8ea151-1fdc-4512-c7e3-92a8e49b703b"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3684/3684 [00:04<00:00, 825.22it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3684, 3684)"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_test = []\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with open(\"pred\", \"w\") as f:\n",
        "    for i in range(len(preprocessed_test_x)):\n",
        "        inputs = preprocessed_test_x[i].to(device)\n",
        "        chars = preprocessed_chars_x[i].to(device)\n",
        "        inputs = inputs.unsqueeze(0)\n",
        "        chars = chars.unsqueeze(0)\n",
        "        outputs = model3(inputs, chars)\n",
        "        predicted_classes = torch.argmax(outputs, dim=2)\n",
        "\n",
        "        tag_count = MAX_LEN\n",
        "        for j in range(len(targets)):\n",
        "            if targets[j].item() == word2idx[\"<pad>\"]:\n",
        "                tag_count = j\n",
        "                break\n",
        "        P = predicted_classes[0][:tag_count].tolist()\n",
        "        original = test_sent[i]\n",
        "\n",
        "        P = P + [tag2idx['O']]*10000\n",
        "        P = P[:len(original)]\n",
        "\n",
        "        for e, p in enumerate(original):\n",
        "            f.write(f\"{p} {idx2tag[P[e]]}\\n\")\n",
        "        f.write(\"\\n\")\n",
        "\n",
        "with open('pred', 'r') as file:\n",
        "    lines = file.readlines()\n",
        "lines.pop()\n",
        "\n",
        "with open('pred', 'w') as file:\n",
        "    file.writelines(lines)\n"
      ],
      "metadata": {
        "id": "O3I1su3hYhH_"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_sent[0]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "huwnUXV1hMTE",
        "outputId": "791094ac-2e55-4354-88f2-bb00532f6c0e"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['1 SOCCER',\n",
              " '2 -',\n",
              " '3 JAPAN',\n",
              " '4 GET',\n",
              " '5 LUCKY',\n",
              " '6 WIN',\n",
              " '7 ,',\n",
              " '8 CHINA',\n",
              " '9 IN',\n",
              " '10 SURPRISE',\n",
              " '11 DEFEAT',\n",
              " '12 .']"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "idx2tag"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KCZtA0-ohRYE",
        "outputId": "115fa6bd-53df-4ebf-e579-15e005a86c32"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'I-PER',\n",
              " 1: 'I-LOC',\n",
              " 2: 'O',\n",
              " 3: 'I-ORG',\n",
              " 4: '<pad>',\n",
              " 5: 'B-PER',\n",
              " 6: 'B-LOC',\n",
              " 7: 'I-MISC',\n",
              " 8: 'B-MISC',\n",
              " 9: 'B-ORG'}"
            ]
          },
          "metadata": {},
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FBwvVLA8Y8NM"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ODv_7fefZzkP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}