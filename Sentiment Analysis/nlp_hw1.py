# -*- coding: utf-8 -*-
"""NLP_HW1

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CHgtufp6dLySLNpK8mOzev5C-uEs_-7E
"""

import sys

print("Python version")
print(sys.version)
print("Version info.")
print(sys.version_info)

import pandas as pd
import numpy as np
import nltk
nltk.download('wordnet')
import re
from bs4 import BeautifulSoup
import requests
import io

"""## Read Data

"""

path = "/content/drive/MyDrive/amazon_reviews_us_Office_Products_v1_00.tsv"
Sentiment_df = pd.read_csv(path, header=0, sep='\t', quotechar='"', on_bad_lines='skip')

"""## Keep Reviews and Ratings"""

columns_selected = ['review_body', 'star_rating']
Sentiment_data = Sentiment_df[columns_selected]

nan_check = Sentiment_data.isna().sum()
print("Number of NaN values in each column:")
print(nan_check)

Sentiment_data = Sentiment_data.dropna()

nan_check = Sentiment_data.isna().sum()
print(nan_check)

Sentiment_data.rename(columns={'review_body': 'Review', 'star_rating': 'Rating'}, inplace=True)

# Select three sample reviews
sample_reviews = Sentiment_data.sample(3)
print("Three Sample Reviews and their Ratings:")
print(sample_reviews)

Sentiment_data['Rating'] = pd.to_numeric(Sentiment_data['Rating'], errors='coerce')

nan_in_rating = Sentiment_data['Rating'].isna().sum()
print("Number of NaN values in 'Rating' column:", nan_in_rating)

# Report statistics of the ratings
ratings_stats = Sentiment_data['Rating'].value_counts().sort_index()
print("\nRatings Statistics:")
for rating in range(1, 6):
    print(f"Number of reviews with rating {rating}: {ratings_stats.get(rating, 0)}")

""" ## We form three classes and select 100000 reviews randomly from each class.


"""

# Create labels for all three classes: 1 for positive (rating > 3), 0 for negative (rating <= 2), and -1 for neutral (rating == 3)
Sentiment_data['Sentiment'] = Sentiment_data['Rating'].apply(lambda x: 1 if x > 3 else (0 if x <= 2 else -1))

positive_count = (Sentiment_data['Sentiment'] == 1).sum()
negative_count = (Sentiment_data['Sentiment'] == 0).sum()
neutral_count = (Sentiment_data['Sentiment'] == -1).sum()

# Print the counts in seperate lines
print("Number of reviews in each class before discarding the neutral reviews")
print("\nNumber of positive reviews (rating > 3):", positive_count)
print("\nNumber of negative reviews (rating <= 2):", negative_count)
print("\nNumber of neutral reviews (rating == 3):", neutral_count)

Sentiment_data.shape

Sentiment_data = Sentiment_data[Sentiment_data['Sentiment'] != -1]

neutral_count_after_removal = (Sentiment_data['Sentiment'] == -1).sum()

# Print the counts in seperate lines
print("\nNumber of reviews in each class after discarding the neutral reviews")
print("\nNumber of positive reviews (rating > 3):", positive_count)
print("\nNumber of negative reviews (rating <= 2):", negative_count)
print("\nNumber of neutral reviews (rating == 3):", neutral_count_after_removal)

Sentiment_data.shape

positive_reviews = Sentiment_data[Sentiment_data['Sentiment'] == 1].sample(n=100000, random_state=42)
negative_reviews = Sentiment_data[Sentiment_data['Sentiment'] == 0].sample(n=100000, random_state=42)

downsized_data = pd.concat([positive_reviews, negative_reviews])
downsized_data.reset_index(drop=True, inplace=True)
print("Shape of the downsized dataset:", downsized_data.shape)

avg_length_before = downsized_data['Review'].apply(len).mean()

"""# Data Cleaning


"""

downsized_data['Review'] = downsized_data['Review'].str.lower()

downsized_data['Review'] = downsized_data['Review'].apply(lambda x: BeautifulSoup(x, "html.parser").get_text())

downsized_data['Review'] = downsized_data['Review'].apply(lambda x: re.sub(r'http\S+', '', x))

contraction_dict = {
    "ain't": "am not / are not / is not / has not / have not",
    "aren't": "are not",
    "can't": "cannot",
    "can't've": "cannot have",
    "'cause": "because",
    "could've": "could have",
    "couldn't": "could not",
    "couldn't've": "could not have",
    "didn't": "did not",
    "doesn't": "does not",
    "don't": "do not",
    "hadn't": "had not",
    "hadn't've": "had not have",
    "hasn't": "has not",
    "haven't": "have not",
    "he'd": "he would / he had",
    "he'd've": "he would have",
    "he'll": "he will",
    "he'll've": "he will have",
    "he's": "he is / he has",
    "how'd": "how did",
    "how'd'y": "how do you",
    "how'll": "how will",
    "how's": "how is / how has / how does",
    "I'd": "I would / I had",
    "I'd've": "I would have",
    "I'll": "I will",
    "I'll've": "I will have",
    "I'm": "I am",
    "I've": "I have",
    "isn't": "is not",
    "it'd": "it would / it had",
    "it'd've": "it would have",
    "it'll": "it will",
    "it'll've": "it will have",
    "it's": "it is / it has",
    "let's": "let us",
    "ma'am": "madam",
    "might've": "might have",
    "mightn't": "might not",
    "mightn't've": "might not have",
    "must've": "must have",
    "mustn't": "must not",
    "mustn't've": "must not have",
    "needn't": "need not",
    "needn't've": "need not have",
    "o'clock": "of the clock",
    "oughtn't": "ought not",
    "oughtn't've": "ought not have",
    "shan't": "shall not",
    "sha'n't": "shall not",
    "shan't've": "shall not have",
    "she'd": "she would / she had",
    "she'd've": "she would have",
    "she'll": "she will",
    "she'll've": "she will have",
    "she's": "she is / she has",
    "should've": "should have",
    "shouldn't": "should not",
    "shouldn't've": "should not have",
    "so've": "so have",
    "so's": "so is / so as",
    "that'd": "that would / that had",
    "that'd've": "that would have",
    "that's": "that is / that has",
    "there'd": "there would / there had",
    "there'd've": "there would have",
    "there's": "there is / there has",
    "they'd": "they would / they had",
    "they'd've": "they would have",
    "they'll": "they will",
    "they'll've": "they will have",
    "they're": "they are",
    "they've": "they have",
    "to've": "to have",
    "wasn't": "was not",
    "we'd": "we would / we had",
    "we'd've": "we would have",
    "we'll": "we will",
    "we'll've": "we will have",
    "we're": "we are",
    "we've": "we have",
    "weren't": "were not",
    "what'll": "what will",
    "what'll've": "what will have",
    "what're": "what are",
    "what's": "what is / what has",
    "what've": "what have",
    "when's": "when is / when has",
    "when've": "when have",
    "where'd": "where did",
    "where's": "where is / where has",
    "where've": "where have",
    "who'll": "who will",
    "who'll've": "who will have",
    "who's": "who is / who has",
    "who've": "who have",
    "why's": "why is / why has",
    "why've": "why have",
    "will've": "will have",
    "won't": "will not",
    "won't've": "will not have",
    "would've": "would have",
    "wouldn't": "would not",
    "wouldn't've": "would not have",
    "y'all": "you all",
    "y'all'd": "you all would",
    "y'all'd've": "you all would have",
    "y'all're": "you all are",
    "y'all've": "you all have",
    "you'd": "you would / you had",
    "you'd've": "you would have",
    "you'll": "you will",
    "you'll've": "you will have",
    "you're": "you are",
    "you've": "you have"
}

def expand_contractions(text, contraction_dict):
    for contraction, expansion in contraction_dict.items():
        text = text.replace(contraction, expansion)
    return text

downsized_data['Review'] = downsized_data['Review'].apply(lambda x: expand_contractions(x, contraction_dict))

downsized_data['Review'] = downsized_data['Review'].str.replace(r'[^a-z\s]', '', regex=True)

downsized_data['Review'] = downsized_data['Review'].str.replace(r'\s+', ' ', regex=True)

avg_length_after = downsized_data['Review'].apply(len).mean()

# Print average review lengths before and after cleaning, separated by a comma
print("Average review length:",
      "Before Data Cleaning:", avg_length_before, ",",
      "After Data Cleaning:", avg_length_after)

"""# Pre-processing"""

avg_length_before_preprocessing_after_data_cleaning = downsized_data['Review'].apply(len).mean()

"""## remove the stop words"""

from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

nltk.download('stopwords')
nltk.download('punkt')

stop_words = set(stopwords.words('english'))

def remove_stopwords(text):
    tokens = word_tokenize(text)
    filtered_tokens = [word for word in tokens if word not in stop_words]
    return ' '.join(filtered_tokens)

downsized_data['Review'] = downsized_data['Review'].apply(remove_stopwords)

"""## perform lemmatization  """

from nltk.stem import WordNetLemmatizer

nltk.download('omw-1.4')

lemmatizer = WordNetLemmatizer()

def lemmatize_text(text):
    tokens = word_tokenize(text)
    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens]
    return ' '.join(lemmatized_tokens)

downsized_data['Review'] = downsized_data['Review'].apply(lemmatize_text)

# Print three sample reviews before preprocessing
print("\nThree sample reviews before data_cleaning + preprocessing:")
print(Sentiment_data['Review'].sample(3))

# Print three sample reviews after preprocessing
print("\nThree sample reviews after data_cleaning + preprocessing:")
print(downsized_data['Review'].sample(3))

avg_length_after = downsized_data['Review'].apply(len).mean()

# Print average review lengths before and after preprocessing, separated by a comma
print("Average review length - preprocessing:",
      "Before preprocessing: ", avg_length_before_preprocessing_after_data_cleaning, ",",
      "After preprocessing:", avg_length_after)

"""# TF-IDF Feature Extraction"""

from sklearn.feature_extraction.text import TfidfVectorizer

tfidf_vectorizer = TfidfVectorizer()

tfidf_features = tfidf_vectorizer.fit_transform(downsized_data['Review'])

features = tfidf_features
labels = downsized_data['Sentiment']

print("Shape of the TF-IDF feature matrix:", features.shape)

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.20, random_state=42)

print("Shape of X_train (features):", X_train.shape)
print("Shape of X_test (features):", X_test.shape)
print("Shape of y_train (labels):", y_train.shape)
print("Shape of y_test (labels):", y_test.shape)

"""# Perceptron"""

from sklearn.linear_model import Perceptron
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

SentiAnlys_perceptron = Perceptron()

SentiAnlys_perceptron.fit(X_train, y_train)

y_train_pred = SentiAnlys_perceptron.predict(X_train)

y_test_pred = SentiAnlys_perceptron.predict(X_test)

print("Training Metrics for Single layer perceptron with coma between them:")
print("Accuracy:", accuracy_score(y_train, y_train_pred), end=", ")
print("Precision:", precision_score(y_train, y_train_pred), end=", ")
print("Recall:", recall_score(y_train, y_train_pred), end=", ")
print("F1 Score:", f1_score(y_train, y_train_pred))

print("\nTraining Metrics for Single layer perceptron with coma between them:")
print("Accuracy:", accuracy_score(y_test, y_test_pred), end=", ")
print("Precision:", precision_score(y_test, y_test_pred), end=", ")
print("Recall:", recall_score(y_test, y_test_pred), end=", ")
print("F1 Score:", f1_score(y_test, y_test_pred))

print("Printing the Performance Metrics for Single Layer Perceptron in seperate lines")
print("\nTraining Metrics:")
print("\nAccuracy:", accuracy_score(y_train, y_train_pred))
print("\nPrecision:", precision_score(y_train, y_train_pred))
print("\nRecall:", recall_score(y_train, y_train_pred))
print("\nF1 Score:", f1_score(y_train, y_train_pred))

print("\nTesting Metrics:")
print("\nAccuracy:", accuracy_score(y_test, y_test_pred))
print("\nPrecision:", precision_score(y_test, y_test_pred))
print("\nRecall:", recall_score(y_test, y_test_pred))
print("\nF1 Score:", f1_score(y_test, y_test_pred))

"""# SVM"""

from sklearn.svm import LinearSVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

SentiAnlys_linear_svm_model = LinearSVC()

SentiAnlys_linear_svm_model.fit(X_train, y_train)

y_train_pred_linear = SentiAnlys_linear_svm_model.predict(X_train)

y_test_pred_linear = SentiAnlys_linear_svm_model.predict(X_test)

print("Linear SVM Training Metrics:", end="")
print(" Accuracy:", accuracy_score(y_train, y_train_pred_linear), end=",")
print(" Precision:", precision_score(y_train, y_train_pred_linear), end=",")
print(" Recall:", recall_score(y_train, y_train_pred_linear), end=",")
print(" F1 Score:", f1_score(y_train, y_train_pred_linear))

print("\nLinear SVM Testing Metrics:", end="")
print(" Accuracy:", accuracy_score(y_test, y_test_pred_linear), end=",")
print(" Precision:", precision_score(y_test, y_test_pred_linear), end=",")
print(" Recall:", recall_score(y_test, y_test_pred_linear), end=",")
print(" F1 Score:", f1_score(y_test, y_test_pred_linear))

print("Printing the Performance Metrics for Linear SVM in seperate lines")
print("\nTraining Metrics:")
print("\nAccuracy:", accuracy_score(y_train, y_train_pred_linear))
print("\nPrecision:", precision_score(y_train, y_train_pred_linear))
print("\nRecall:", recall_score(y_train, y_train_pred_linear))
print("\nF1 Score:", f1_score(y_train, y_train_pred_linear))

print("\nTesting Metrics:")
print("\nAccuracy:", accuracy_score(y_test, y_test_pred_linear))
print("\nPrecision:", precision_score(y_test, y_test_pred_linear))
print("\nRecall:", recall_score(y_test, y_test_pred_linear))
print("\nF1 Score:", f1_score(y_test, y_test_pred_linear))

"""# Logistic Regression"""

from sklearn.preprocessing import MaxAbsScaler

scaler = MaxAbsScaler()

X_train_scaled = scaler.fit_transform(X_train)

X_test_scaled = scaler.transform(X_test)

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

SentiAnlys_logistic_model = LogisticRegression(max_iter=1000)  # Increase max_iter as needed

SentiAnlys_logistic_model.fit(X_train_scaled, y_train)

y_train_pred_logistic = SentiAnlys_logistic_model.predict(X_train_scaled)

y_test_pred_logistic = SentiAnlys_logistic_model.predict(X_test_scaled)

print("Logistic Regression Training Metrics:", end="")
print(" Accuracy:", accuracy_score(y_train, y_train_pred_logistic), end=",")
print(" Precision:", precision_score(y_train, y_train_pred_logistic), end=",")
print(" Recall:", recall_score(y_train, y_train_pred_logistic), end=",")
print(" F1 Score:", f1_score(y_train, y_train_pred_logistic))

print("\nLogistic Regression Testing Metrics:", end="")
print(" Accuracy:", accuracy_score(y_test, y_test_pred_logistic), end=",")
print(" Precision:", precision_score(y_test, y_test_pred_logistic), end=",")
print(" Recall:", recall_score(y_test, y_test_pred_logistic), end=",")
print(" F1 Score:", f1_score(y_test, y_test_pred_logistic))

print("Printing the Performance Metrics for logistric regression in seperate lines")
print("\nTraining Metrics:")
print("\nAccuracy:", accuracy_score(y_train, y_train_pred_logistic))
print("\nPrecision:", precision_score(y_train, y_train_pred_logistic))
print("\nRecall:", recall_score(y_train, y_train_pred_logistic))
print("\nF1 Score:", f1_score(y_train, y_train_pred_logistic))

print("\nTesting Metrics:")
print("\nAccuracy:", accuracy_score(y_test, y_test_pred_logistic))
print("\nPrecision:", precision_score(y_test, y_test_pred_logistic))
print("\nRecall:", recall_score(y_test, y_test_pred_logistic))
print("\nF1 Score:", f1_score(y_test, y_test_pred_logistic))

"""# Naive Bayes"""

from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

SentiAnlys_nb_model = MultinomialNB()

SentiAnlys_nb_model.fit(X_train, y_train)

y_train_pred_nb = SentiAnlys_nb_model.predict(X_train)

y_test_pred_nb = SentiAnlys_nb_model.predict(X_test)

print("Multinomial Naive Bayes Training Metrics:", end="")
print(" Accuracy:", accuracy_score(y_train, y_train_pred_nb), end=",")
print(" Precision:", precision_score(y_train, y_train_pred_nb), end=",")
print(" Recall:", recall_score(y_train, y_train_pred_nb), end=",")
print(" F1 Score:", f1_score(y_train, y_train_pred_nb))

print("\nMultinomial Naive Bayes Testing Metrics:", end="")
print(" Accuracy:", accuracy_score(y_test, y_test_pred_nb), end=",")
print(" Precision:", precision_score(y_test, y_test_pred_nb), end=",")
print(" Recall:", recall_score(y_test, y_test_pred_nb), end=",")
print(" F1 Score:", f1_score(y_test, y_test_pred_nb))

print("Printing the Performance Metrics for Multinomial Naive Bayes in seperate lines")
print("\nTraining Metrics:")
print("\nAccuracy:", accuracy_score(y_train, y_train_pred_nb))
print("\nPrecision:", precision_score(y_train, y_train_pred_nb))
print("\nRecall:", recall_score(y_train, y_train_pred_nb))
print("\nF1 Score:", f1_score(y_train, y_train_pred_nb))

print("Testing Metrics:")
print("\nAccuracy:", accuracy_score(y_test, y_test_pred_nb))
print("\nPrecision:", precision_score(y_test, y_test_pred_nb))
print("\nRecall:", recall_score(y_test, y_test_pred_nb))
print("\nF1 Score:", f1_score(y_test, y_test_pred_nb))

