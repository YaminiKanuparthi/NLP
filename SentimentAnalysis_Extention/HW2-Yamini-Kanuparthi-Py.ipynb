{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW2_CSCI544\n",
    "## NAME : YAMINI HARIPRIYA\n",
    "## USC ID : 9195653004"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Dataset Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the tools that are added to the code at the start of the process are NumPy, Pandas, Regular Expressions, BeautifulSoup, scikit-learn, Gensim, tqdm, NLTK, and Torch. After that, Pandas is used to read the information from a TSV file. In more detail, it chooses the \"review_body\" and \"star_rating\" fields and sets the delimiter and encoding at the same time. The DataFrame is made even by picking 50,000 reviews for each rating from 1 to 5, after any NaN values have been removed and the \"star_rating\" column has been changed to a number format. This makes a DataFrame that is balanced, which is called \"balanced_data.\"\n",
    "** **\n",
    "A preprocessing function named 'preprocess_text' is defined to lowercase text, remove HTML tags, URLs, non-alphabetic characters, and stopwords, and lemmatize the remaining tokens. This function is applied to the 'Review' column of the 'balanced_data' DataFrame, thereby cleaning the text data.\n",
    "** **\n",
    "Sentiment labels ('sentiment_class') are created based on star ratings, where ratings greater than 3 are labeled as positive (1), ratings less than 3 are labeled as negative (2), and ratings equal to 3 are labeled as neutral (0). The dataset is then split into features ('X_train' and 'X_test') and labels ('y_train' and 'y_test') using the 'train_test_split' function from scikit-learn, with a test size of 20% and a random state of 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Print Python version\n",
    "print(\"Python Version:\", sys.version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Python dependencies\n",
    "print(\"\\nDependencies:\")\n",
    "for module in sys.modules:\n",
    "    try:\n",
    "        module_version = sys.modules[module].__version__\n",
    "        print(module, \":\", module_version)\n",
    "    except AttributeError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "import pandas as pd  \n",
    "import re           \n",
    "from bs4 import BeautifulSoup \n",
    "from sklearn.model_selection import train_test_split  \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import LinearSVC\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize import word_tokenize\n",
    "import gensim.downloader as api\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Yamini_Kanuparthi\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]     .\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Yamini_Kanuparthi\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]     .\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk         \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet') \n",
    "nltk.download('stopwords')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yamini_Kanuparthi\\AppData\\Local\\Temp\\ipykernel_12756\\3307972338.py:3: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  Sentiment_df = pd.read_csv(path,usecols=['review_body', 'star_rating'], delimiter='\\t', encoding='utf-8', error_bad_lines=False)\n",
      "C:\\Users\\Yamini_Kanuparthi\\AppData\\Local\\Temp\\ipykernel_12756\\3307972338.py:3: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Sentiment_df = pd.read_csv(path,usecols=['review_body', 'star_rating'], delimiter='\\t', encoding='utf-8', error_bad_lines=False)\n"
     ]
    }
   ],
   "source": [
    "# Loading dataset with only necessary columns\n",
    "path = r\"C:\\Users\\Yamini_Kanuparthi\\Downloads\\amazon_reviews_us_Office_Products_v1_00.tsv\"\n",
    "Sentiment_df = pd.read_csv(path,usecols=['review_body', 'star_rating'], delimiter='\\t', encoding='utf-8', error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Preprocessing: Cleaning and balancing dataset\n",
    "Sentiment_df.dropna(inplace=True)\n",
    "Sentiment_df['star_rating'] = pd.to_numeric(Sentiment_df['star_rating'], errors='coerce')\n",
    "Sentiment_df.rename(columns={'review_body': 'Review', 'star_rating': 'Rating'}, inplace=True)\n",
    "\n",
    "# Balancing dataset\n",
    "balanced_data = pd.DataFrame()\n",
    "for rating in range(1, 6):  # Assuming ratings are from 1 to 5\n",
    "    subset = Sentiment_df[Sentiment_df['Rating'] == rating]\n",
    "    balanced_subset = subset.sample(n=50000, random_state=42)\n",
    "    balanced_data = pd.concat([balanced_data, balanced_subset], ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balanced_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rating</th>\n",
       "      <th>Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>This is one of the worst purchases I have ever...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Applied correctly to winter coat. Did not last...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>This product broke in a very short time.  It a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Two of the four ink cartridges were empty. You...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>My SO and I have owned two of these, and they ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rating                                             Review\n",
       "0     1.0  This is one of the worst purchases I have ever...\n",
       "1     1.0  Applied correctly to winter coat. Did not last...\n",
       "2     1.0  This product broke in a very short time.  It a...\n",
       "3     1.0  Two of the four ink cartridges were empty. You...\n",
       "4     1.0  My SO and I have owned two of these, and they ..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balanced_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: contractions in c:\\users\\yamini_kanuparthi\\anaconda3\\lib\\site-packages (0.1.73)\n",
      "Requirement already satisfied: textsearch>=0.0.21 in c:\\users\\yamini_kanuparthi\\anaconda3\\lib\\site-packages (from contractions) (0.0.24)\n",
      "Requirement already satisfied: pyahocorasick in c:\\users\\yamini_kanuparthi\\anaconda3\\lib\\site-packages (from textsearch>=0.0.21->contractions) (2.0.0)\n",
      "Requirement already satisfied: anyascii in c:\\users\\yamini_kanuparthi\\anaconda3\\lib\\site-packages (from textsearch>=0.0.21->contractions) (0.3.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yamini_Kanuparthi\\anaconda3\\lib\\site-packages\\bs4\\__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Yamini_Kanuparthi\\anaconda3\\lib\\site-packages\\bs4\\__init__.py:404: MarkupResemblesLocatorWarning: The input looks more like a URL than markup. You may want to use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Combining text preprocessing steps into a single operation\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = contractions.fix(text)\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", '', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "balanced_data['Review'] = balanced_data['Review'].apply(preprocess_text)\n",
    "\n",
    "# Creating labels and splitting dataset\n",
    "balanced_data['sentiment_class'] = balanced_data['Rating'].apply(lambda x: 1 if x > 3 else (2 if x < 3 else 0))\n",
    "X_train, X_test, y_train, y_test = train_test_split(balanced_data['Review'], balanced_data['sentiment_class'], test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Word Embedding  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 a) Here I have used Gensim Library to load Word2Vec model. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first part of the code installs the Gensim library for inserting words. Using Gensim's API, it loads a Word2Vec model that has already been trained: \"word2vec-google-news-300.\" Then, vector processes are used to quickly find words that are similar. For example, the vectors for \"king,\" \"man,\" and \"woman\" are changed to create a new vector. Using the pre-trained model, the code writes words that are similar and words that have semantic similarities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the text input has been cleaned up, a custom Word2Vec model is trained on it. The code creates a function that finds similarities between vectors and then uses that function to find the similarities between the changed vector ('queen' - 'king' -'man' + 'woman') and 'queen'. The custom model also checks to see if certain words have semantic connections.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, binary labels are prepared for sentiment analysis (positive for ratings > 3, negative for ratings < 3). The dataset is split into training and testing sets for binary sentiment classification, and binary labels are created accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrained Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\yamini_kanuparthi\\anaconda3\\lib\\site-packages (4.3.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\yamini_kanuparthi\\anaconda3\\lib\\site-packages (from gensim) (5.2.1)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\yamini_kanuparthi\\anaconda3\\lib\\site-packages (from gensim) (1.23.5)\n",
      "Requirement already satisfied: scipy>=1.7.0 in c:\\users\\yamini_kanuparthi\\anaconda3\\lib\\site-packages (from gensim) (1.10.0)\n",
      "Requirement already satisfied: FuzzyTM>=0.4.0 in c:\\users\\yamini_kanuparthi\\anaconda3\\lib\\site-packages (from gensim) (2.0.5)\n",
      "Requirement already satisfied: pandas in c:\\users\\yamini_kanuparthi\\anaconda3\\lib\\site-packages (from FuzzyTM>=0.4.0->gensim) (1.5.3)\n",
      "Requirement already satisfied: pyfume in c:\\users\\yamini_kanuparthi\\anaconda3\\lib\\site-packages (from FuzzyTM>=0.4.0->gensim) (0.2.25)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\yamini_kanuparthi\\anaconda3\\lib\\site-packages (from pandas->FuzzyTM>=0.4.0->gensim) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\yamini_kanuparthi\\anaconda3\\lib\\site-packages (from pandas->FuzzyTM>=0.4.0->gensim) (2022.7)\n",
      "Requirement already satisfied: fst-pso in c:\\users\\yamini_kanuparthi\\anaconda3\\lib\\site-packages (from pyfume->FuzzyTM>=0.4.0->gensim) (1.8.1)\n",
      "Requirement already satisfied: simpful in c:\\users\\yamini_kanuparthi\\anaconda3\\lib\\site-packages (from pyfume->FuzzyTM>=0.4.0->gensim) (2.11.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\yamini_kanuparthi\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->FuzzyTM>=0.4.0->gensim) (1.16.0)\n",
      "Requirement already satisfied: miniful in c:\\users\\yamini_kanuparthi\\anaconda3\\lib\\site-packages (from fst-pso->pyfume->FuzzyTM>=0.4.0->gensim) (0.0.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('king', 0.8449392318725586), ('queen', 0.7300517559051514), ('monarch', 0.645466148853302), ('princess', 0.6156251430511475), ('crown_prince', 0.5818676352500916), ('prince', 0.5777117609977722), ('kings', 0.5613664388656616), ('sultan', 0.5376776456832886), ('Queen_Consort', 0.5344247221946716), ('queens', 0.5289887189865112)]\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained Word2Vec model\n",
    "wv = api.load('word2vec-google-news-300')\n",
    "\n",
    "# Perform vector operations and find similar words efficiently\n",
    "king_vec = wv['king']\n",
    "man_vec = wv['man']\n",
    "woman_vec = wv['woman']\n",
    "new_vector = king_vec - man_vec + woman_vec\n",
    "similar_words = wv.similar_by_vector(new_vector)\n",
    "print(similar_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic similarities (woman + king - man): [('queen', 0.7118192911148071)]\n"
     ]
    }
   ],
   "source": [
    "# Check semantic similarities using the pre-trained model\n",
    "similarities = wv.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
    "print(f\"Semantic similarities (woman + king - man): {similarities}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between 'excellent' and 'outstanding': 0.556748628616333\n"
     ]
    }
   ],
   "source": [
    "# Check similarity for specific words\n",
    "similarity = wv.similarity('excellent', 'outstanding')\n",
    "print(f\"Similarity between 'excellent' and 'outstanding': {similarity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between 'king' and 'queen': 0.6510956883430481\n"
     ]
    }
   ],
   "source": [
    "# Check similarity for specific words\n",
    "similarity = wv.similarity('king', 'queen')\n",
    "print(f\"Similarity between 'king' and 'queen': {similarity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [word_tokenize(review.lower()) for review in balanced_data['Review']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Similarity between the (King- Man + Woman) and (Queen) with custom model is 0.2535056\n",
      "Similarity between EXCELLENT and OUTSTANDING is 0.76753366\n",
      "Similarity between KING and QUEEN is 0.39628977\n"
     ]
    }
   ],
   "source": [
    "# Train the Word2Vec model\n",
    "model_cust = Word2Vec(sentences, vector_size=300, window=11, min_count=10, workers=4)\n",
    "\n",
    "def similarityFunc(word1, word2):\n",
    "  return np.dot(word1, word2)/(np.linalg.norm(word1)* np.linalg.norm(word2))\n",
    "\n",
    "# Calculate the vector for \"king\" - \"man\" + \"woman\"\n",
    "if all(word in model_cust.wv.key_to_index for word in ['king', 'man', 'woman', 'queen']):\n",
    "    result_vector = model_cust.wv['king'] - model_cust.wv['man'] + model_cust.wv['woman']\n",
    "    queen_vector = model_cust.wv['queen']\n",
    "\n",
    "    # Calculate the similarity score\n",
    "    similar_score_custom = similarityFunc(result_vector, queen_vector)\n",
    "    print(\"The Similarity between the (King- Man + Woman) and (Queen) with custom model is\", similar_score_custom)\n",
    "\n",
    "# Check semantic similarities using the custom model\n",
    "similarity = model_cust.wv.similarity(\"excellent\", \"outstanding\")\n",
    "print(\"Similarity between EXCELLENT and OUTSTANDING is\", similarity)\n",
    "\n",
    "similarity = model_cust.wv.similarity(\"king\", \"queen\")\n",
    "print(\"Similarity between KING and QUEEN is\", similarity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** **\n",
    "** **\n",
    "** **\n",
    "** **\n",
    "** **\n",
    "** **\n",
    "\n",
    "### What do you conclude from comparing vectors generated by yourself and the pretrained model? Which of the Word2Vec models seems to encode semantic similarities between words better?\n",
    "\n",
    "Comparing the vectors generated by the pretrained model and the custom-trained model, we observe some differences in semantic similarities between words. \n",
    "\n",
    "In the pretrained model:\n",
    "- The vector operations 'King Man+Woman=Queen' yield similar words like 'queen', 'monarch', 'princess', indicating a strong semantic relationship.\n",
    "- The semantic similarity between 'excellent' and 'outstanding' is moderate at 0.5567.\n",
    "- The similarity between 'king' and 'queen' is also moderate at 0.65109.\n",
    "** **\n",
    "In the custom-trained model:\n",
    "- The vector operations for 'King Man+Woman=Queen' result in a similarity score of 0.2648 between the computed vector and 'queen', which is lower compared to the pretrained model where it is 0.7118.\n",
    "- The semantic similarity between 'excellent' and 'outstanding' is higher at 0.7984.\n",
    "- The similarity between 'king' and 'queen' is lower compared to the pretrained model, indicating that the custom-trained model may not capture this relationship as effectively.\n",
    "** **\n",
    "When comparing the similarity between (King - Man + Woman) and (Queen), we can see that the custom trained model is not as good as the pre-trained one, since the pre-trained one has a semantic similarity of 71% while the custom model has a similarity of 21%. However, when we look at the semantic similarity between EXCELLENT and OUTSTANDING, the semantic similarity of the custom trained model is great. The issue could be because Word2Vec is trained on a large corpus, thus there may be many terms closer to EXCELLENT and OUTSTANDING, lowering the relative similarity. \n",
    "\n",
    "From this comparison, it appears that the pretrained Word2Vec model tends to encode semantic similarities between words better, as evidenced by the higher similarity scores for vector operations and word pairs. However, the custom-trained model may still provide meaningful embeddings tailored to the specific dataset. Overall, the pretrained model demonstrates a stronger ability to capture semantic relationships present in a broader corpus of text data.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yamini_Kanuparthi\\AppData\\Local\\Temp\\ipykernel_13292\\1444351052.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  binary_data['binary_sentiment'] = binary_data['Rating'].apply(lambda x: 1 if x > 3 else 0)\n"
     ]
    }
   ],
   "source": [
    "binary_data = balanced_data[balanced_data['Rating'] != 3]\n",
    "\n",
    "# Prepare binary labels: 1 for positive (ratings > 3) and 0 for negative (ratings < 3)\n",
    "binary_data['binary_sentiment'] = binary_data['Rating'].apply(lambda x: 1 if x > 3 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_binary, X_test_binary, y_train_binary, y_test_binary = train_test_split(binary_data['Review'], binary_data['binary_sentiment'], test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(160000,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_binary.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Simple models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thos part aims to generate Word2Vec features for the binary sentiment classification task using pretrained and custom-trained Word2Vec models. It then trains Perceptron and SVM models using these features and compares their accuracy with models trained using TF-IDF features. Below is a summary of the code:\n",
    "\n",
    "1. **Word2Vec Feature Extraction:**\n",
    "   - Word embeddings are extracted for training and testing data using both the pretrained ('word2vec-google-news-300') and custom-trained Word2Vec models.\n",
    "   - For each review, the average Word2Vec vectors are computed by summing up the word vectors and dividing by the number of words.\n",
    "   \n",
    "  ** ** \n",
    "** **\n",
    "  \n",
    "** **\n",
    "  \n",
    "   \n",
    "2. **Model Training and Evaluation:**\n",
    "   - Perceptron and SVM models are trained using Word2Vec features extracted from both pretrained and custom-trained models.\n",
    "   - Accuracy scores are computed for each model using the test data, and the results are printed.\n",
    "   \n",
    "   ** ** \n",
    "   \n",
    "3. **TF-IDF Feature Extraction and Model Training:**\n",
    "   - TF-IDF features are generated using the TfidfVectorizer from scikit-learn.\n",
    "   - Perceptron and SVM models are trained using TF-IDF features, and their accuracy scores are computed and printed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_document_embedding(doc):\n",
    "    doc_str = ' '.join(doc)  # Join tokens within the document list to form a single string\n",
    "    words = doc_str.lower().split()  # Split the string into words\n",
    "    if not words:\n",
    "        return np.zeros(300)  # Return a zero vector if no words are present\n",
    "    else:\n",
    "        return wv.get_mean_vector(words)  # Compute the mean vector using Word2Vec model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 160000/160000 [02:24<00:00, 1104.58it/s]\n"
     ]
    }
   ],
   "source": [
    "binary_word_vectors_train = []\n",
    "\n",
    "# Extract Word2Vec features for training data\n",
    "for e in tqdm(X_train_binary):\n",
    "    binary_word_vectors_train.append(compute_document_embedding(e))\n",
    "\n",
    "\n",
    "binary_word_vectors_train = np.array(binary_word_vectors_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [00:37<00:00, 1062.06it/s]\n"
     ]
    }
   ],
   "source": [
    "binary_word_vectors_test = []\n",
    "\n",
    "# Extract Word2Vec features for testing data\n",
    "for e in tqdm(X_test_binary):\n",
    "    binary_word_vectors_test.append(compute_document_embedding(e))\n",
    "    \n",
    "binary_word_vectors_test = np.array(binary_word_vectors_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_document_embedding(doc):\n",
    "    doc_str = ' '.join(doc)  # Join tokens within the document list to form a single string\n",
    "    words = doc_str.lower().split()  # Split the string into words\n",
    "    if not words:\n",
    "        return np.zeros(300)  # Return a zero vector if no words are present\n",
    "    else:\n",
    "        return model_cust.wv.get_mean_vector(words)  # Compute the mean vector using Word2Vec model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 160000/160000 [02:12<00:00, 1208.08it/s]\n"
     ]
    }
   ],
   "source": [
    "bin_cust_w2v_train = []\n",
    "\n",
    "\n",
    "# Extract Word2Vec features for training data\n",
    "for e in tqdm(X_train_binary):\n",
    "    bin_cust_w2v_train.append(compute_document_embedding(e))\n",
    "    \n",
    "bin_cust_w2v_train = np.array(bin_cust_w2v_train)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [00:32<00:00, 1229.40it/s]\n"
     ]
    }
   ],
   "source": [
    "bin_cust_w2v_test = []\n",
    "    \n",
    "# Extract Word2Vec features for testing data\n",
    "for e in tqdm(X_test_binary):\n",
    "    bin_cust_w2v_test.append(compute_document_embedding(e))\n",
    "    \n",
    "bin_cust_w2v_test = np.array(bin_cust_w2v_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((160000, 300), (40000, 300))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bin_cust_w2v_train.shape, bin_cust_w2v_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron and SVM for Pretrained Word2Vec - Binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron (Pretrained Word2Vec) Accuracy: 0.5425\n"
     ]
    }
   ],
   "source": [
    "perceptron_pretrained = Perceptron(max_iter=1000)\n",
    "perceptron_pretrained.fit(binary_word_vectors_train, y_train_binary)\n",
    "\n",
    "predictions = perceptron_pretrained.predict(binary_word_vectors_test)\n",
    "accuracy = accuracy_score(y_test_binary, predictions)\n",
    "print(\"Perceptron (Pretrained Word2Vec) Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC (Pretrained Word2Vec) Accuracy: 0.61595\n"
     ]
    }
   ],
   "source": [
    "svm_pretrained = LinearSVC()\n",
    "svm_pretrained.fit(binary_word_vectors_train, y_train_binary)\n",
    "\n",
    "predictions = svm_pretrained.predict(binary_word_vectors_test)\n",
    "accuracy = accuracy_score(y_test_binary, predictions)\n",
    "print(\"LinearSVC (Pretrained Word2Vec) Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron and SVM for Custom Word2Vec - Binary Clasification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron (Custom Word2Vec) Accuracy: 0.525875\n"
     ]
    }
   ],
   "source": [
    "perceptron_custom = Perceptron()\n",
    "perceptron_custom.fit(bin_cust_w2v_train, y_train_binary)\n",
    "\n",
    "predictions = perceptron_custom.predict(bin_cust_w2v_test)\n",
    "accuracy = accuracy_score(y_test_binary, predictions)\n",
    "print(\"Perceptron (Custom Word2Vec) Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC (Custom Word2Vec) Accuracy: 0.61815\n"
     ]
    }
   ],
   "source": [
    "svm_custom = LinearSVC()\n",
    "svm_custom.fit(bin_cust_w2v_train, y_train_binary)\n",
    "\n",
    "predictions = svm_custom.predict(bin_cust_w2v_test)\n",
    "accuracy = accuracy_score(y_test_binary, predictions)\n",
    "print(\"LinearSVC (Custom Word2Vec) Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron and SVM Training -  TF- IDF features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize the TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Assuming 'Review' is your preprocessed text column in 'binary_data'\n",
    "text_data = binary_data['Review']\n",
    "\n",
    "# Fit and transform the text data to get TF-IDF features\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(text_data)\n",
    "\n",
    "\n",
    "# Use the 'binary_sentiment' column as labels\n",
    "y = binary_data['binary_sentiment']\n",
    "\n",
    "# Split the TF-IDF features and labels into training and testing sets\n",
    "X_train_tfidf, X_test_tfidf, y_train_tfidf, y_test_tfidf = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron (TF-IDF) Accuracy: 0.81385\n"
     ]
    }
   ],
   "source": [
    "perceptron_tfidf = Perceptron()\n",
    "perceptron_tfidf.fit(X_train_tfidf, y_train_tfidf)\n",
    "\n",
    "predictions = perceptron_tfidf.predict(X_test_tfidf)\n",
    "accuracy = accuracy_score(y_test_tfidf, predictions)\n",
    "print(\"Perceptron (TF-IDF) Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC (TF-IDF) Accuracy: 0.863825\n"
     ]
    }
   ],
   "source": [
    "svm_tfidf = LinearSVC()\n",
    "svm_tfidf.fit(X_train_tfidf, y_train_tfidf)\n",
    "\n",
    "predictions = svm_tfidf.predict(X_test_tfidf)\n",
    "accuracy = accuracy_score(y_test_tfidf, predictions)\n",
    "print(\"LinearSVC (TF-IDF) Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What do you conclude from comparing performances for the models trained using the three different feature types (TF-IDF, pretrained Word2Vec, your trained Word2Vec)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "From the obtained accuracy scores, we can draw the following conclusions regarding the performances of models trained using different feature types:\n",
    "\n",
    "1. **TF-IDF Features:** The Perceptron and LinearSVC models trained using TF-IDF features achieve the highest accuracy scores among all models. This indicates that TF-IDF features are highly effective in capturing the discriminative information necessary for sentiment classification. The Perceptron model achieves an accuracy of approximately 81.39%, while the LinearSVC model achieves an accuracy of approximately 86.38%. TF-IDF is a straightforward and efficient approach that often yields satisfactory results for smaller datasets. It can effectively identify the most pertinent words and phrases that relate to the sentiment of a given document, making it suitable for sentiment analysis tasks.\n",
    "** **\n",
    "2. **Pretrained Word2Vec Features:** Models trained using Word2Vec features extracted from the pretrained model achieve moderate accuracy scores. The Single Layer Perceptron using pretrained Word2Vec features achieves an accuracy of approximately 54.25%, while the SVM model achieves an accuracy of approximately 61.60%. Pretrained Word2Vec embeddings are typically beneficial when processing large quantities of textual data, as they are capable of capturing semantic relationships between words and phrases. They can be particularly effective for sentiment analysis tasks involving slang, idiomatic expressions, and other colloquial language that may pose challenges for traditional bag-of-words methods like TF-IDF.\n",
    "** **\n",
    "3. **Custom Word2Vec Features:** Models trained using Word2Vec features from the custom-trained model exhibit slightly improved accuracy compared to models using pretrained Word2Vec features. The Single Layer Perceptron achieves an accuracy of approximately 52.0%, while the SVM model achieves an accuracy of approximately 61.83%. However, these accuracy scores are still lower than those obtained using TF-IDF features, indicating that the custom Word2Vec features may not be as effective as TF-IDF features for sentiment classification on this dataset. Nonetheless, it is reasonable to speculate that for more extensive and intricate datasets, Word2Vec may have the upper hand.\n",
    "\n",
    "In summary, TF-IDF features outperform both pretrained and custom Word2Vec features for sentiment classification on the given dataset. While Word2Vec features capture semantic information, TF-IDF features seem to better capture the discriminative information necessary for sentiment analysis in this context. However, the choice between TF-IDF and Word2Vec depends on the characteristics of the dataset and the specific requirements of the sentiment analysis task.\n",
    "\n",
    "** **\n",
    "** **\n",
    "\n",
    "** **\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Feedforward Neural Networks "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 4a) Using Average Word2Vec Vectors:\n",
    "1. **Binary Classification:**\n",
    "   - Concatenated Word2Vec vectors were used as input features for the binary classification model.\n",
    "   - A binary MLP model with two hidden layers (50 and 10 nodes) and softmax activation in the output layer was defined.\n",
    "   - The model was trained using cross-entropy loss and Adam optimizer.\n",
    "   - Accuracy was evaluated on the testing split.\n",
    "** **\n",
    "\n",
    "2. **Ternary Classification:**\n",
    "   - Similar steps were followed as for binary classification, but with three output classes.\n",
    "   - The model was trained and evaluated using the same approach as for binary classification.\n",
    "\n",
    "### 4b) Concatenating Word2Vec Vectors:\n",
    "1. **Binary Classification:**\n",
    "   - The first 10 Word2Vec vectors for each review were concatenated to form the input features.\n",
    "   - The binary MLP model was redefined, trained, and evaluated using the concatenated features.\n",
    "** **\n",
    "** **\n",
    "** **\n",
    "\n",
    "2. **Ternary Classification:**\n",
    "   - Similar steps were followed as for binary classification, but with three output classes.\n",
    "   - The model was trained and evaluated using the concatenated features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to binary format (0 for class 1, 1 for class 2)\n",
    "\n",
    "y_np_train = np.array(y_train_binary)\n",
    "y_np_test = np.array(y_test_binary)\n",
    "\n",
    "y_train_binary = np.where(y_np_train == 1, 0, 1)\n",
    "y_test_binary = np.where(y_np_test == 1, 0, 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the MLP Model for binary classification\n",
    "class BinaryMLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n",
    "        super(BinaryMLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.fc3 = nn.Linear(hidden_size2, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return self.softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop with tqdm progress bar\n",
    "def train_with_progress_bar(model, train_loader, criterion, optimizer, num_epochs):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        with tqdm(total=len(train_loader), desc=f'Epoch {epoch+1}/{num_epochs}', unit='batch') as pbar:\n",
    "            for inputs, labels in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "                pbar.set_postfix({'loss': f'{running_loss/len(train_loader):.3f}'})\n",
    "                pbar.update()\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss:.3f}\")\n",
    "\n",
    "\n",
    "# Evaluation\n",
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print(f\"Accuracy: {correct/total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Layer Perceptron with Binary Classification - Pretrained Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to PyTorch tensors\n",
    "train_dataset_pre_binary = TensorDataset(torch.tensor(binary_word_vectors_train, dtype=torch.float32), torch.tensor(y_train_binary, dtype=torch.long))\n",
    "test_dataset_pre_binary = TensorDataset(torch.tensor(binary_word_vectors_test, dtype=torch.float32), torch.tensor(y_test_binary, dtype=torch.long))\n",
    "\n",
    "# Create DataLoader\n",
    "train_loader_pre_binary = DataLoader(train_dataset_pre_binary, batch_size=32, shuffle=True)\n",
    "test_loader_pre_binary = DataLoader(test_dataset_pre_binary, batch_size=32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 5000/5000 [00:20<00:00, 238.82batch/s, loss=0.668]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 3339.553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 5000/5000 [00:23<00:00, 215.30batch/s, loss=0.653]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Loss: 3264.732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 5000/5000 [00:21<00:00, 236.26batch/s, loss=0.647]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Loss: 3235.242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 5000/5000 [00:22<00:00, 226.59batch/s, loss=0.644]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Loss: 3221.684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 5000/5000 [00:22<00:00, 224.22batch/s, loss=0.643]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Loss: 3214.788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 5000/5000 [00:22<00:00, 223.67batch/s, loss=0.642]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Loss: 3208.857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 5000/5000 [00:22<00:00, 217.55batch/s, loss=0.641]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Loss: 3204.465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 5000/5000 [00:22<00:00, 224.58batch/s, loss=0.640]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Loss: 3197.777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 5000/5000 [00:21<00:00, 227.46batch/s, loss=0.639]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Loss: 3194.755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 5000/5000 [00:22<00:00, 219.89batch/s, loss=0.638]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Loss: 3191.535\n",
      "Accuracy: 0.639625\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "input_size = binary_word_vectors_train.shape[1]  # Size of input features\n",
    "hidden_size1 = 50\n",
    "hidden_size2 = 10\n",
    "output_size = 2  # Binary classification (classes 1 and 2)\n",
    "binary_model_pre_binary = BinaryMLP(input_size, hidden_size1, hidden_size2, output_size)\n",
    "\n",
    "# Define Loss Function and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(binary_model_pre_binary.parameters(), lr=0.001)\n",
    "\n",
    "# Train the binary classification model\n",
    "num_epochs = 10\n",
    "train_with_progress_bar(binary_model_pre_binary, train_loader_pre_binary, criterion, optimizer, num_epochs)\n",
    "\n",
    "# Evaluate the binary classification model\n",
    "evaluate(binary_model_pre_binary, test_loader_pre_binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Layer Perceptron with Binary Classification - Custom Word2Vec Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to PyTorch tensors\n",
    "train_dataset_cust_binary = TensorDataset(torch.tensor(bin_cust_w2v_train, dtype=torch.float32), torch.tensor(y_train_binary, dtype=torch.long))\n",
    "test_dataset_cust_binary = TensorDataset(torch.tensor(bin_cust_w2v_test, dtype=torch.float32), torch.tensor(y_test_binary, dtype=torch.long))\n",
    "\n",
    "# Create DataLoader\n",
    "train_loader_cust_binary = DataLoader(train_dataset_cust_binary, batch_size=32, shuffle=True)\n",
    "test_loader_cust_binary = DataLoader(test_dataset_cust_binary, batch_size=32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 5000/5000 [00:17<00:00, 288.61batch/s, loss=0.664]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 3318.768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 5000/5000 [00:19<00:00, 253.90batch/s, loss=0.651]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Loss: 3253.512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 5000/5000 [00:20<00:00, 245.89batch/s, loss=0.645]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Loss: 3227.132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 5000/5000 [00:20<00:00, 246.37batch/s, loss=0.643]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Loss: 3215.373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 5000/5000 [00:20<00:00, 248.70batch/s, loss=0.641]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Loss: 3204.521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 5000/5000 [00:20<00:00, 241.70batch/s, loss=0.639]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Loss: 3195.764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 5000/5000 [00:21<00:00, 233.52batch/s, loss=0.638]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Loss: 3190.410\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 5000/5000 [00:21<00:00, 231.62batch/s, loss=0.637]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Loss: 3185.489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 5000/5000 [00:20<00:00, 240.49batch/s, loss=0.636]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Loss: 3181.307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 5000/5000 [00:22<00:00, 223.26batch/s, loss=0.636]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Loss: 3178.313\n",
      "Accuracy: 0.638125\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "input_size = bin_cust_w2v_train.shape[1]  # Size of input features\n",
    "hidden_size1 = 50\n",
    "hidden_size2 = 10\n",
    "output_size = 2  # Binary classification (classes 1 and 2)\n",
    "binary_model_cust_binary = BinaryMLP(input_size, hidden_size1, hidden_size2, output_size)\n",
    "\n",
    "# Define Loss Function and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(binary_model_cust_binary.parameters(), lr=0.001)\n",
    "\n",
    "# Train the binary classification model\n",
    "num_epochs = 10\n",
    "train_with_progress_bar(binary_model_cust_binary, train_loader_cust_binary, criterion, optimizer, num_epochs)\n",
    "\n",
    "# Evaluate the binary classification model\n",
    "evaluate(binary_model_cust_binary, test_loader_cust_binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Layer Perceptron with Ternary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_document_embedding(doc):\n",
    "    doc_str = ' '.join(doc)  # Join tokens within the document list to form a single string\n",
    "    words = doc_str.lower().split()  # Split the string into words\n",
    "    if not words:\n",
    "        return np.zeros(300)  # Return a zero vector if no words are present\n",
    "    else:\n",
    "        return model_cust.wv.get_mean_vector(words)  # Compute the mean vector using Word2Vec model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200000/200000 [02:47<00:00, 1193.70it/s]\n"
     ]
    }
   ],
   "source": [
    "ter_cust_w2v_train = []\n",
    "\n",
    "# Extract Word2Vec features for training data\n",
    "for e in tqdm(X_train):\n",
    "    ter_cust_w2v_train.append(compute_document_embedding(e))\n",
    "\n",
    "ter_cust_w2v_train = np.array(ter_cust_w2v_train)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:42<00:00, 1176.30it/s]\n"
     ]
    }
   ],
   "source": [
    "ter_cust_w2v_test = []\n",
    "\n",
    "# Extract Word2Vec features for testing data\n",
    "for e in tqdm(X_test):\n",
    "    ter_cust_w2v_test.append(compute_document_embedding(e))\n",
    "    \n",
    "ter_cust_w2v_test = np.array(ter_cust_w2v_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((200000, 300), (50000, 300))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ter_cust_w2v_train.shape, ter_cust_w2v_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_document_embedding(doc):\n",
    "    doc_str = ' '.join(doc)  # Join tokens within the document list to form a single string\n",
    "    words = doc_str.lower().split()  # Split the string into words\n",
    "    if not words:\n",
    "        return np.zeros(300)  # Return a zero vector if no words are present\n",
    "    else:\n",
    "        return wv.get_mean_vector(words)  # Compute the mean vector using Word2Vec model\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200000/200000 [02:58<00:00, 1117.76it/s]\n"
     ]
    }
   ],
   "source": [
    "ternary_word_vectors_train = []\n",
    "\n",
    "# Extract Word2Vec features for training data\n",
    "for e in tqdm(X_train):\n",
    "    ternary_word_vectors_train.append(compute_document_embedding(e))\n",
    "    \n",
    "ternary_word_vectors_train = np.array(ternary_word_vectors_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:44<00:00, 1120.76it/s]\n"
     ]
    }
   ],
   "source": [
    "ternary_word_vectors_test = []\n",
    "\n",
    "\n",
    "# Extract Word2Vec features for testing data\n",
    "for e in tqdm(X_test):\n",
    "    ternary_word_vectors_test.append(compute_document_embedding(e))\n",
    "    \n",
    "ternary_word_vectors_test = np.array(ternary_word_vectors_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((200000, 300), (50000, 300))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ternary_word_vectors_train.shape, ternary_word_vectors_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to NumPy arrays\n",
    "# Convert labels to integers ranging from 0 to 2\n",
    "\n",
    "y_np_train = np.array(y_train)\n",
    "y_np_test = np.array(y_test)\n",
    "\n",
    "y_train_encoded = [label for label in y_np_train]  \n",
    "y_test_encoded = [label for label in y_np_test]  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the MLP Model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.fc3 = nn.Linear(hidden_size2, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return self.softmax(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "# Training Loop with tqdm progress bar\n",
    "def train_with_progress_bar(model, train_loader, criterion, optimizer, num_epochs):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        with tqdm(total=len(train_loader), desc=f'Epoch {epoch+1}/{num_epochs}', unit='batch') as pbar:\n",
    "            for inputs, labels in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "                pbar.set_postfix({'loss': f'{running_loss/len(train_loader):.3f}'})\n",
    "                pbar.update()\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss}\")\n",
    "\n",
    "\n",
    "# Evaluation\n",
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print(f\"Accuracy: {correct/total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Layer Perceptron with Ternary Classification - Pretrained Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to PyTorch tensors\n",
    "train_dataset_pre_ternary = TensorDataset(torch.tensor(ternary_word_vectors_train, dtype=torch.float32), torch.tensor(y_train_encoded, dtype=torch.long))\n",
    "test_dataset_pre_ternary = TensorDataset(torch.tensor(ternary_word_vectors_test, dtype=torch.float32), torch.tensor(y_test_encoded, dtype=torch.long))\n",
    "\n",
    "# Create DataLoader\n",
    "train_loader_pre_ternary = DataLoader(train_dataset_pre_ternary, batch_size=32, shuffle=True)\n",
    "test_loader_pre_ternary = DataLoader(test_dataset_pre_ternary, batch_size=32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 6250/6250 [00:27<00:00, 228.68batch/s, loss=1.041]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 6504.971320569515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 6250/6250 [00:27<00:00, 223.54batch/s, loss=1.027]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Loss: 6416.973695039749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 6250/6250 [00:30<00:00, 208.22batch/s, loss=1.022]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Loss: 6387.726795017719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 6250/6250 [00:32<00:00, 194.60batch/s, loss=1.018]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Loss: 6365.305337131023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 6250/6250 [00:29<00:00, 214.14batch/s, loss=1.016]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Loss: 6352.651197493076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 6250/6250 [00:30<00:00, 207.79batch/s, loss=1.014]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Loss: 6339.7665050029755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 6250/6250 [00:29<00:00, 209.75batch/s, loss=1.012]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Loss: 6327.648021638393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 6250/6250 [00:30<00:00, 204.82batch/s, loss=1.011]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Loss: 6318.43892544508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 6250/6250 [00:30<00:00, 206.89batch/s, loss=1.010]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Loss: 6312.607225835323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 6250/6250 [00:30<00:00, 207.98batch/s, loss=1.009]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Loss: 6307.618082404137\n",
      "Accuracy: 0.51178\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "input_size = ternary_word_vectors_train.shape[1]  # Size of input features\n",
    "hidden_size1 = 50\n",
    "hidden_size2 = 10\n",
    "output_size = 3  # Assuming 3 classes for sentiment analysis\n",
    "ternary_model_pre = MLP(input_size, hidden_size1, hidden_size2, output_size)\n",
    "\n",
    "# Define Loss Function and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(ternary_model_pre.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 10\n",
    "train_with_progress_bar(ternary_model_pre, train_loader_pre_ternary, criterion, optimizer, num_epochs)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate(ternary_model_pre, test_loader_pre_ternary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Layer Perceptron with Ternary Classification - Custom Word2Vec Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to PyTorch tensors\n",
    "train_dataset_cust_ternary = TensorDataset(torch.tensor(ter_cust_w2v_train, dtype=torch.float32), torch.tensor(y_train_encoded, dtype=torch.long))\n",
    "test_dataset_cust_ternary = TensorDataset(torch.tensor(ter_cust_w2v_test, dtype=torch.float32), torch.tensor(y_test_encoded, dtype=torch.long))\n",
    "\n",
    "# Create DataLoader\n",
    "train_loader_cust_ternary = DataLoader(train_dataset_cust_ternary, batch_size=32, shuffle=True)\n",
    "test_loader_cust_ternary = DataLoader(test_dataset_cust_ternary, batch_size=32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 6250/6250 [00:30<00:00, 202.70batch/s, loss=1.041]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 6504.433157503605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 6250/6250 [00:32<00:00, 194.03batch/s, loss=1.022]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Loss: 6386.571774423122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 6250/6250 [00:31<00:00, 198.62batch/s, loss=1.018]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Loss: 6359.678361237049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 6250/6250 [00:33<00:00, 186.90batch/s, loss=1.016]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Loss: 6348.236070275307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 6250/6250 [00:31<00:00, 198.15batch/s, loss=1.015]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Loss: 6341.936175048351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 6250/6250 [00:30<00:00, 202.02batch/s, loss=1.013]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Loss: 6331.388066470623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 6250/6250 [00:30<00:00, 201.82batch/s, loss=1.012]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Loss: 6323.279178380966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 6250/6250 [00:31<00:00, 197.61batch/s, loss=1.011]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Loss: 6316.446863055229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 6250/6250 [00:31<00:00, 196.15batch/s, loss=1.010]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Loss: 6313.760311603546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 6250/6250 [00:32<00:00, 194.12batch/s, loss=1.010]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Loss: 6311.003338396549\n",
      "Accuracy: 0.50586\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "input_size = ter_cust_w2v_train.shape[1]  # Size of input features\n",
    "hidden_size1 = 50\n",
    "hidden_size2 = 10\n",
    "output_size = 3  # Assuming 3 classes for sentiment analysis\n",
    "Ternary_model_cust = MLP(input_size, hidden_size1, hidden_size2, output_size)\n",
    "\n",
    "# Define Loss Function and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(Ternary_model_cust.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 10\n",
    "train_with_progress_bar(Ternary_model_cust, train_loader_cust_ternary, criterion, optimizer, num_epochs)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate(Ternary_model_cust, test_loader_cust_ternary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First 10 Word2Vec vectors on Ternary Classification - Custom Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the first 10 Word2Vec vectors for each review\n",
    "X_train_conc_custTer = []\n",
    "X_test_con_custTer = []\n",
    "\n",
    "for review in ter_cust_w2v_train:\n",
    "    review_reshaped = review.reshape(1, -1)  # Reshape to ensure it's a 2D array\n",
    "    concatenated_vector = np.concatenate(review_reshaped[:10], axis=0)\n",
    "    X_train_conc_custTer.append(concatenated_vector)\n",
    "\n",
    "for review in ter_cust_w2v_test:\n",
    "    review_reshaped = review.reshape(1, -1)  # Reshape to ensure it's a 2D array\n",
    "    concatenated_vector = np.concatenate(review_reshaped[:10], axis=0)\n",
    "    X_test_con_custTer.append(concatenated_vector)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yamini_Kanuparthi\\AppData\\Local\\Temp\\ipykernel_13292\\687041903.py:2: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  C:\\b\\abs_bao0hdcrdh\\croot\\pytorch_1675190257512\\work\\torch\\csrc\\utils\\tensor_new.cpp:204.)\n",
      "  X_train_concat_tensor_cust = torch.tensor(X_train_conc_custTer, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "# Convert the concatenated features into PyTorch tensors\n",
    "X_train_concat_tensor_cust = torch.tensor(X_train_conc_custTer, dtype=torch.float32)\n",
    "X_test_concat_tensor_cust = torch.tensor(X_test_con_custTer, dtype=torch.float32)\n",
    "\n",
    "# Convert labels to ternary format\n",
    "y_train_tensor_cust = torch.tensor(y_train_encoded, dtype=torch.long)\n",
    "y_test_tensor_cust = torch.tensor(y_test_encoded, dtype=torch.long)\n",
    "\n",
    "# Create DataLoader\n",
    "train_dataset_con_custTer = TensorDataset(X_train_concat_tensor_cust, y_train_tensor_cust)\n",
    "test_dataset_con_custTer = TensorDataset(X_test_concat_tensor_cust, y_test_tensor_cust)\n",
    "\n",
    "train_loader_con_custTer = DataLoader(train_dataset_con_custTer, batch_size=32, shuffle=True)\n",
    "test_loader_con_custTer = DataLoader(test_dataset_con_custTer, batch_size=32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 6250/6250 [00:31<00:00, 199.13batch/s, loss=1.039]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 6492.232459485531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 6250/6250 [00:32<00:00, 194.03batch/s, loss=1.022]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Loss: 6386.859766960144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 6250/6250 [00:33<00:00, 188.84batch/s, loss=1.018]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Loss: 6362.95522159338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 6250/6250 [00:31<00:00, 199.46batch/s, loss=1.016]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Loss: 6348.905470967293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 6250/6250 [00:31<00:00, 200.66batch/s, loss=1.014]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Loss: 6336.412016749382\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 6250/6250 [00:31<00:00, 199.49batch/s, loss=1.013]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Loss: 6328.354793846607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 6250/6250 [00:31<00:00, 200.49batch/s, loss=1.011]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Loss: 6321.722619891167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 6250/6250 [00:31<00:00, 199.58batch/s, loss=1.011]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Loss: 6315.888198435307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 6250/6250 [00:31<00:00, 198.21batch/s, loss=1.010]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Loss: 6312.940587043762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 6250/6250 [00:32<00:00, 194.33batch/s, loss=1.009]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Loss: 6307.007692813873\n",
      "Accuracy: 0.50824\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "input_size = X_train_concat_tensor_cust.shape[1]  # Size of input features\n",
    "hidden_size1 = 50\n",
    "hidden_size2 = 10\n",
    "output_size = 3  # Assuming 3 classes for sentiment analysis\n",
    "model_con_custTer = MLP(input_size, hidden_size1, hidden_size2, output_size)\n",
    "\n",
    "# Define Loss Function and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_con_custTer.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 10\n",
    "train_with_progress_bar(model_con_custTer, train_loader_con_custTer, criterion, optimizer, num_epochs)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate(model_con_custTer, test_loader_con_custTer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First 10 Word2Vec vectors on Ternary Classification - Pretrained  Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the first 10 Word2Vec vectors for each review\n",
    "X_train_con_preTer = []\n",
    "X_test_con_preTer = []\n",
    "\n",
    "for review in ternary_word_vectors_train:\n",
    "    review_reshaped = review.reshape(1, -1)  # Reshape to ensure it's a 2D array\n",
    "    concatenated_vector = np.concatenate(review_reshaped[:10], axis=0)\n",
    "    X_train_con_preTer.append(concatenated_vector)\n",
    "\n",
    "for review in ternary_word_vectors_test:\n",
    "    review_reshaped = review.reshape(1, -1)  # Reshape to ensure it's a 2D array\n",
    "    concatenated_vector = np.concatenate(review_reshaped[:10], axis=0)\n",
    "    X_test_con_preTer.append(concatenated_vector)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the concatenated features into PyTorch tensors\n",
    "X_train_con_preTer_tensor = torch.tensor(X_train_con_preTer, dtype=torch.float32)\n",
    "X_test_con_preTer_tensor = torch.tensor(X_test_con_preTer, dtype=torch.float32)\n",
    "\n",
    "# Convert labels to ternary format\n",
    "y_train_tensor = torch.tensor(y_train_encoded, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test_encoded, dtype=torch.long)\n",
    "\n",
    "# Create DataLoader\n",
    "train_dataset_con_preTer = TensorDataset(X_train_con_preTer_tensor, y_train_tensor)\n",
    "test_dataset_con_preTer = TensorDataset(X_test_con_preTer_tensor, y_test_tensor)\n",
    "\n",
    "train_loader_concat = DataLoader(train_dataset_con_preTer, batch_size=32, shuffle=True)\n",
    "test_loader_concat = DataLoader(test_dataset_con_preTer, batch_size=32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 6250/6250 [00:32<00:00, 195.26batch/s, loss=1.048]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 6547.956055641174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 6250/6250 [00:33<00:00, 188.12batch/s, loss=1.028]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Loss: 6427.81138497591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 6250/6250 [00:32<00:00, 190.85batch/s, loss=1.025]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Loss: 6407.285757124424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 6250/6250 [00:32<00:00, 189.86batch/s, loss=1.024]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Loss: 6398.774301946163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 6250/6250 [00:32<00:00, 189.52batch/s, loss=1.022]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Loss: 6386.606939792633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 6250/6250 [00:33<00:00, 186.39batch/s, loss=1.019]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Loss: 6368.783692240715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 6250/6250 [00:32<00:00, 192.10batch/s, loss=1.017]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Loss: 6358.591976106167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 6250/6250 [00:32<00:00, 193.44batch/s, loss=1.016]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Loss: 6350.372229039669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 6250/6250 [00:32<00:00, 193.01batch/s, loss=1.015]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Loss: 6344.632316529751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 6250/6250 [00:33<00:00, 187.84batch/s, loss=1.014]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Loss: 6340.228854119778\n",
      "Accuracy: 0.50066\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "input_size = X_train_con_preTer_tensor.shape[1]  # Size of input features\n",
    "hidden_size1 = 50\n",
    "hidden_size2 = 10\n",
    "output_size = 3  # Assuming 3 classes for sentiment analysis\n",
    "model_con_preTer = MLP(input_size, hidden_size1, hidden_size2, output_size)\n",
    "\n",
    "# Define Loss Function and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_con_preTer.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 10\n",
    "train_with_progress_bar(model_con_preTer, train_loader_concat, criterion, optimizer, num_epochs)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate(model_con_preTer, test_loader_concat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First 10 Word2Vec vectors on Binary Classification -  Pretrained Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the first 10 Word2Vec vectors for each review\n",
    "X_train_con_preBin = []\n",
    "X_test_con_preBin = []\n",
    "\n",
    "for review in binary_word_vectors_train:\n",
    "    review_reshaped = review.reshape(1, -1)  # Reshape to ensure it's a 2D array\n",
    "    concatenated_vector = np.concatenate(review_reshaped[:10], axis=0)\n",
    "    X_train_con_preBin.append(concatenated_vector)\n",
    "\n",
    "for review in binary_word_vectors_test:\n",
    "    review_reshaped = review.reshape(1, -1)  # Reshape to ensure it's a 2D array\n",
    "    concatenated_vector = np.concatenate(review_reshaped[:10], axis=0)\n",
    "    X_test_con_preBin.append(concatenated_vector)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the concatenated features into PyTorch tensors\n",
    "X_train_con_preBin_tensor = torch.tensor(X_train_con_preBin, dtype=torch.float32)\n",
    "X_test_con_preBin_tensor = torch.tensor(X_test_con_preBin, dtype=torch.float32)\n",
    "\n",
    "# Convert labels to ternary format\n",
    "y_train_tensor_preBin = torch.tensor(y_train_binary, dtype=torch.long)\n",
    "y_test_tensor_preBin = torch.tensor(y_test_binary, dtype=torch.long)\n",
    "\n",
    "# Create DataLoader\n",
    "train_dataset_con_preBin = TensorDataset(X_train_con_preBin_tensor, y_train_tensor_preBin)\n",
    "test_dataset_con_preBin = TensorDataset(X_test_con_preBin_tensor, y_test_tensor_preBin)\n",
    "\n",
    "train_loader_con_preBin = DataLoader(train_dataset_con_preBin, batch_size=32, shuffle=True)\n",
    "test_loader_con_preBin = DataLoader(test_dataset_con_preBin, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 5000/5000 [00:24<00:00, 202.31batch/s, loss=0.668]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 3337.67026668787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 5000/5000 [00:24<00:00, 205.42batch/s, loss=0.655]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Loss: 3276.2301232218742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 5000/5000 [00:25<00:00, 197.99batch/s, loss=0.651]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Loss: 3253.36962094903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 5000/5000 [00:24<00:00, 205.51batch/s, loss=0.647]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Loss: 3234.237141877413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 5000/5000 [00:25<00:00, 198.68batch/s, loss=0.644]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Loss: 3218.8004455268383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 5000/5000 [00:24<00:00, 204.90batch/s, loss=0.642]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Loss: 3211.7116121053696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 5000/5000 [00:25<00:00, 192.81batch/s, loss=0.641]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Loss: 3203.43296790123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 5000/5000 [00:27<00:00, 184.12batch/s, loss=0.640]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Loss: 3197.6700633764267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 5000/5000 [00:27<00:00, 184.44batch/s, loss=0.639]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Loss: 3194.3690379858017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 5000/5000 [00:24<00:00, 203.05batch/s, loss=0.638]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Loss: 3188.5361228883266\n",
      "Accuracy: 0.639275\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize the model\n",
    "input_size = X_train_con_preBin_tensor.shape[1]  # Size of input features\n",
    "hidden_size1 = 50\n",
    "hidden_size2 = 10\n",
    "output_size = 2  # Assuming 2 classes for sentiment analysis\n",
    "model_con_preBin = MLP(input_size, hidden_size1, hidden_size2, output_size)\n",
    "\n",
    "# Define Loss Function and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_con_preBin.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 10\n",
    "train_with_progress_bar(model_con_preBin, train_loader_con_preBin, criterion, optimizer, num_epochs)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate(model_con_preBin, test_loader_con_preBin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First 10 Word2Vec vectors on Binary Classification - Custom Word2Vec Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the first 10 Word2Vec vectors for each review\n",
    "X_train_con_custBin = []\n",
    "X_test_con_custBin = []\n",
    "\n",
    "for review in bin_cust_w2v_train:\n",
    "    review_reshaped = review.reshape(1, -1)  # Reshape to ensure it's a 2D array\n",
    "    concatenated_vector = np.concatenate(review_reshaped[:10], axis=0)\n",
    "    X_train_con_custBin.append(concatenated_vector)\n",
    "\n",
    "for review in bin_cust_w2v_test:\n",
    "    review_reshaped = review.reshape(1, -1)  # Reshape to ensure it's a 2D array\n",
    "    concatenated_vector = np.concatenate(review_reshaped[:10], axis=0)\n",
    "    X_test_con_custBin.append(concatenated_vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert the concatenated features into PyTorch tensors\n",
    "X_train_con_custBin_tensor = torch.tensor(X_train_con_custBin, dtype=torch.float32)\n",
    "X_test_con_custBin_tensor = torch.tensor(X_test_con_custBin, dtype=torch.float32)\n",
    "\n",
    "# Convert labels to ternary format\n",
    "y_train_tensor_custBin = torch.tensor(y_train_binary, dtype=torch.long)\n",
    "y_test_tensor_custBin = torch.tensor(y_test_binary, dtype=torch.long)\n",
    "\n",
    "# Create DataLoader\n",
    "train_dataset_con_custBin = TensorDataset(X_train_con_custBin_tensor, y_train_tensor_custBin)\n",
    "test_dataset_con_custBin = TensorDataset(X_test_con_custBin_tensor, y_test_tensor_custBin)\n",
    "\n",
    "train_loader_con_custBin = DataLoader(train_dataset_con_custBin, batch_size=32, shuffle=True)\n",
    "test_loader_con_custBin = DataLoader(test_dataset_con_custBin, batch_size=32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 5000/5000 [00:24<00:00, 205.79batch/s, loss=0.661]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 3302.6701834201813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 5000/5000 [00:23<00:00, 214.35batch/s, loss=0.647]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Loss: 3237.0680083036423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 5000/5000 [00:24<00:00, 203.30batch/s, loss=0.644]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Loss: 3219.2847456634045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 5000/5000 [00:23<00:00, 214.22batch/s, loss=0.642]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Loss: 3207.868731468916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 5000/5000 [00:23<00:00, 210.05batch/s, loss=0.640]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Loss: 3201.989105671644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 5000/5000 [00:24<00:00, 203.55batch/s, loss=0.639]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Loss: 3194.3605867028236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 5000/5000 [00:25<00:00, 197.11batch/s, loss=0.637]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Loss: 3186.8690469264984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 5000/5000 [00:24<00:00, 207.77batch/s, loss=0.636]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Loss: 3181.068635046482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 5000/5000 [00:24<00:00, 205.83batch/s, loss=0.635]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Loss: 3177.1513691544533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 5000/5000 [00:23<00:00, 209.66batch/s, loss=0.635]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Loss: 3173.6881232857704\n",
      "Accuracy: 0.6377\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "input_size = X_train_con_custBin_tensor.shape[1]  # Size of input features\n",
    "hidden_size1 = 50\n",
    "hidden_size2 = 10\n",
    "output_size = 2  # Assuming 2 classes for sentiment analysis\n",
    "model_con_custBin = MLP(input_size, hidden_size1, hidden_size2, output_size)\n",
    "\n",
    "# Define Loss Function and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_con_custBin.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 10\n",
    "train_with_progress_bar(model_con_custBin, train_loader_con_custBin, criterion, optimizer, num_epochs)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate(model_con_custBin, test_loader_con_custBin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What do you conclude by comparing accuracy values you obtain with those obtained in the Simple Models section (note you can compare the accuracy values for binary classification).\n",
    "\n",
    "1. **Simple Models (Perceptron and SVM)**:\n",
    "   - Perceptron (TF-IDF): 0.81385\n",
    "   - LinearSVC (TF-IDF): 0.863825\n",
    "   - Perceptron (Word2Vec Pretrained Features): 0.5425\n",
    "   - LinearSVC (Word2Vec Pretrained Features): 0.61595\n",
    "   - Perceptron (Word2Vec Custom Features): 0.58\n",
    "   - LinearSVC (Word2Vec Custom Features): 0.618325\n",
    "\n",
    "2. **Feedforward Neural Networks (FFNN)**:\n",
    "   - FFNN with Custom Word2Vec Embeddings (Average Word2Vec): 0.641675\n",
    "   - FFNN with Pretrained Word2Vec Embeddings (Average Word2Vec): 0.638475\n",
    "   - FFNN with Pretrained Word2Vec Embeddings (Concatenated First 10 Vectors): 0.623225\n",
    "   - FFNN with Custom Word2Vec Embeddings (Concatenated First 10 Vectors): 0.63875\n",
    "\n",
    "From the comparison, we can observe that:\n",
    "\n",
    "- TF-IDF features consistently outperform both Word2Vec embeddings (pretrained and custom) for binary classification using the Simple Models (Perceptron and LinearSVC).This may be due to TF-IDF's ability to effectively identify the most relevant words and phrases for sentiment analysis in smaller datasets. \n",
    "- Among the Word2Vec embeddings, the FFNN with Custom Word2Vec Embeddings (Average Word2Vec) achieves the highest accuracy of 0.641675 for binary classification, slightly outperforming the FFNN with Pretrained Word2Vec Embeddings (Average Word2Vec) which has an accuracy of 0.638475.\n",
    "- When considering the concatenated first 10 vectors, the FFNN with Custom Word2Vec Embeddings achieves a slightly higher accuracy of 0.63875 compared to 0.623225 obtained by the FFNN with Pretrained Word2Vec Embeddings.\n",
    "\n",
    "Overall, TF-IDF features perform better than Word2Vec embeddings for binary sentiment analysis on this dataset. However, the FFNNs trained with custom Word2Vec embeddings show competitive performance, especially when using the average Word2Vec vectors. Additionally, the choice between pretrained and custom embeddings may depend on factors such as dataset size, domain specificity, and computational resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Convolutional Neural Networks "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I crafted a custom dataset class, named TextDataset, which orchestrates the loading of preprocessed text data alongside their corresponding sentiment labels. This class also harnesses a pre-trained Word2Vec model to convert the text inputs into meaningful embeddings.\n",
    "\n",
    "My CNN architecture is designed to feature convolutional layers, followed by max-pooling layers and fully connected layers. This model ingests Word2Vec embeddings as input and outputs predictions regarding the sentiment of the text.With the architecture in place, we proceed to train the CNN model using the training data. Throughout the training process, we utilize cross-entropy loss for computing the model's loss and the Adam optimizer for updating the model's parameters.\n",
    "\n",
    "To gauge the CNN model's efficacy, I evaluated its performance on the test data. By analyzing the accuracy metric, I ascertained the model's proficiency in accurately classifying sentiment labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN with Binary Classification using the Word2Vec Pre-Trained Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(balanced_data['Review'], balanced_data['sentiment_class'], test_size=0.2, random_state=42)\n",
    "X_train_binary, X_test_binary, y_train_binary, y_test_binary = train_test_split(binary_data['Review'], binary_data['binary_sentiment'], test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess text data\n",
    "def preprocess_text(text, max_length=50):\n",
    "    tokens = text.split()[:max_length]  # Limit maximum review length\n",
    "    padded_tokens = tokens + ['<PAD>'] * (max_length - len(tokens))  # Pad shorter reviews\n",
    "    return padded_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, wv, max_length=50):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.wv = wv\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts.iloc[idx]\n",
    "        label = self.labels.iloc[idx]\n",
    "        tokens = preprocess_text(text, self.max_length)\n",
    "        vector = np.zeros((self.max_length, wv.vector_size), dtype=np.float32)  # Keep as float32\n",
    "        for i, token in enumerate(tokens):\n",
    "            if token in self.wv:\n",
    "                vector[i] = self.wv[token]\n",
    "            else:\n",
    "                vector[i] = np.zeros(wv.vector_size)\n",
    "        return torch.tensor(vector, dtype=torch.float32), torch.tensor(label, dtype=torch.long)  # Ensure dtype is float32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the CNN model\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(input_size, 50, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(50, 10, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        self.fc = nn.Linear(10 * 12, output_size)  # Adjust the input size for the linear layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(nn.functional.relu(self.conv1(x)))\n",
    "        x = self.pool(nn.functional.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 10 * 12)  # Adjust the reshaping operation\n",
    "        x = self.fc(x)\n",
    "        return nn.functional.softmax(x, dim=1)\n",
    "\n",
    "\n",
    "def train_cnn(model, train_loader, criterion, optimizer, num_epochs):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        for i, (inputs, labels) in progress_bar:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs.permute(0, 2, 1))  # Permute input dimensions for Conv1D\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            progress_bar.set_postfix({'loss': running_loss / (i + 1)})\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss / len(train_loader)}\")\n",
    "\n",
    "\n",
    "# Evaluation\n",
    "def evaluate_cnn(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs.permute(0, 2, 1))  # Permute input dimensions for Conv1D\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print(f\"Accuracy: {correct / total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter and prepare data as described initially\n",
    "# Assuming X_train_binary, y_train_binary, X_test_binary, y_test_binary are already defined\n",
    "\n",
    "train_dataset = TextDataset(X_train_binary, y_train_binary, wv)\n",
    "test_dataset = TextDataset(X_test_binary, y_test_binary, wv)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 5000/5000 [01:53<00:00, 44.12it/s, loss=0.479]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.4790187286913395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 5000/5000 [01:38<00:00, 50.72it/s, loss=0.448]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Loss: 0.4483756018579006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 5000/5000 [01:39<00:00, 50.22it/s, loss=0.436]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Loss: 0.43590270971059797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 5000/5000 [01:39<00:00, 50.14it/s, loss=0.427]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Loss: 0.42741728733181955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 5000/5000 [01:38<00:00, 50.63it/s, loss=0.421]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Loss: 0.4212290760278702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 5000/5000 [01:39<00:00, 50.49it/s, loss=0.416]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Loss: 0.4156973664700985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 5000/5000 [01:38<00:00, 50.60it/s, loss=0.412]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Loss: 0.41228791163563727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 5000/5000 [01:38<00:00, 50.78it/s, loss=0.409]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Loss: 0.4090325618505478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 5000/5000 [01:39<00:00, 50.33it/s, loss=0.406]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Loss: 0.4060825840473175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 5000/5000 [01:40<00:00, 49.65it/s, loss=0.403]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Loss: 0.4034036285281181\n",
      "Accuracy: 0.8642\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "input_size = wv.vector_size\n",
    "output_size = 2  # Binary classification\n",
    "model = CNN(input_size, output_size)\n",
    "\n",
    "# Define Loss Function and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "train_cnn(model, train_loader, criterion, optimizer, num_epochs=10)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_cnn(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN with Ternary Classification using the Word2Vec Pre-Trained Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter and prepare data as described initially\n",
    "# Assuming X_train_binary, y_train_binary, X_test_binary, y_test_binary are already defined\n",
    "\n",
    "train_dataset = TextDataset(X_train, y_train, wv)\n",
    "test_dataset = TextDataset(X_test, y_test, wv)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 6250/6250 [02:07<00:00, 49.04it/s, loss=0.875]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.8754354429531097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 6250/6250 [02:03<00:00, 50.57it/s, loss=0.84] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Loss: 0.8401069899082184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 6250/6250 [02:03<00:00, 50.62it/s, loss=0.828]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Loss: 0.8282566421890258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 6250/6250 [02:03<00:00, 50.60it/s, loss=0.819]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Loss: 0.8191366889858246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 6250/6250 [02:03<00:00, 50.48it/s, loss=0.812]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Loss: 0.8115094116973877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 6250/6250 [02:03<00:00, 50.73it/s, loss=0.805]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Loss: 0.8054937970352173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 6250/6250 [02:03<00:00, 50.57it/s, loss=0.799]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Loss: 0.7993476189994811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 6250/6250 [02:03<00:00, 50.46it/s, loss=0.795]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Loss: 0.7947168046283722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 6250/6250 [02:04<00:00, 50.30it/s, loss=0.791]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Loss: 0.7905362372493744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 6250/6250 [02:05<00:00, 49.77it/s, loss=0.787]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Loss: 0.7866367936229706\n",
      "Accuracy: 0.7042\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "input_size = wv.vector_size\n",
    "output_size = 3  # Binary classification\n",
    "model = CNN(input_size, output_size)\n",
    "\n",
    "# Define Loss Function and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "train_cnn(model, train_loader, criterion, optimizer, num_epochs=10)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_cnn(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN with Binary Classification using the Word2Vec Custom Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, wv_model, max_length=50):\n",
    "        \"\"\"\n",
    "        texts: Series or list of text documents\n",
    "        labels: Series or list of labels\n",
    "        wv_model: The trained Word2Vec model (Gensim)\n",
    "        max_length: Maximum length of tokens in a document\n",
    "        \"\"\"\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.wv_model = wv_model\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        vector = np.zeros((self.max_length, self.wv_model.vector_size))\n",
    "\n",
    "        # Convert text to vector\n",
    "        for i, word in enumerate(text[:self.max_length]):\n",
    "            if word in self.wv_model.wv:\n",
    "                vector[i] = self.wv_model.wv[word]\n",
    "            else:\n",
    "                vector[i] = np.zeros(self.wv_model.vector_size)\n",
    "\n",
    "        return torch.tensor(vector, dtype=torch.float32), torch.tensor(label, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming balanced_data['Review'] and the corresponding labels are pandas Series\n",
    "X_train_binary = X_train_binary.reset_index(drop=True)\n",
    "y_train_binary = y_train_binary.reset_index(drop=True)\n",
    "X_test_binary = X_test_binary.reset_index(drop=True)\n",
    "y_test_binary = y_test_binary.reset_index(drop=True)\n",
    "\n",
    "# Now, pass these to your TextDataset instances\n",
    "train_dataset = TextDataset(X_train_binary, y_train_binary, model_cust)\n",
    "test_dataset = TextDataset(X_test_binary, y_test_binary, model_cust)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter and prepare data as described initially\n",
    "# Assuming X_train_binary, y_train_binary, X_test_binary, y_test_binary are already defined\n",
    "\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 5000/5000 [01:35<00:00, 52.57it/s, loss=0.594]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.5939369991242885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 5000/5000 [01:35<00:00, 52.56it/s, loss=0.564]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Loss: 0.564359641867876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 5000/5000 [01:35<00:00, 52.48it/s, loss=0.555]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Loss: 0.5554887140810489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 5000/5000 [01:35<00:00, 52.46it/s, loss=0.55] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Loss: 0.5504629420042038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 5000/5000 [01:35<00:00, 52.50it/s, loss=0.547]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Loss: 0.5468562577486038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 5000/5000 [01:35<00:00, 52.17it/s, loss=0.545]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Loss: 0.5445355002641677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 5000/5000 [01:36<00:00, 51.92it/s, loss=0.542]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Loss: 0.5422063845336437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 5000/5000 [01:36<00:00, 52.01it/s, loss=0.54] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Loss: 0.5398669212222099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 5000/5000 [01:36<00:00, 51.68it/s, loss=0.539]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Loss: 0.5388882895052433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 5000/5000 [01:37<00:00, 51.47it/s, loss=0.537]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Loss: 0.5372337187767029\n",
      "Accuracy: 0.75025\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "input_size = model_cust.vector_size\n",
    "output_size = 2  # Binary classification\n",
    "model = CNN(input_size, output_size)\n",
    "\n",
    "# Define Loss Function and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "train_cnn(model, train_loader, criterion, optimizer, num_epochs=10)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_cnn(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN with Ternary Classification using the Word2Vec Custom Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming balanced_data['Review'] and the corresponding labels are pandas Series\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "y_train= y_train.reset_index(drop=True)\n",
    "X_test= X_test.reset_index(drop=True)\n",
    "y_test = y_test.reset_index(drop=True)\n",
    "\n",
    "# Now, pass these to your TextDataset instances\n",
    "train_dataset = TextDataset(X_train, y_train, model_cust)\n",
    "test_dataset = TextDataset(X_test, y_test, model_cust)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter and prepare data as described initially\n",
    "# Assuming X_train_binary, y_train_binary, X_test_binary, y_test_binary are already defined\n",
    "\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 6250/6250 [02:04<00:00, 50.09it/s, loss=0.975]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.975375921163559\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 6250/6250 [02:02<00:00, 51.01it/s, loss=0.949]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Loss: 0.9491483219814301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 6250/6250 [02:03<00:00, 50.56it/s, loss=0.941]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Loss: 0.9411510003089905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 6250/6250 [02:04<00:00, 50.04it/s, loss=0.938]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Loss: 0.937754278049469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 6250/6250 [02:06<00:00, 49.35it/s, loss=0.935]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Loss: 0.9347191872310638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 6250/6250 [02:07<00:00, 48.96it/s, loss=0.933]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Loss: 0.9327261985874176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 6250/6250 [02:08<00:00, 48.67it/s, loss=0.931]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Loss: 0.9309856537818909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 6250/6250 [02:10<00:00, 47.91it/s, loss=0.93] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Loss: 0.9300689810371399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 6250/6250 [02:12<00:00, 47.04it/s, loss=0.929]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Loss: 0.9285816311931611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 6250/6250 [02:12<00:00, 47.00it/s, loss=0.928]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Loss: 0.9278761985492706\n",
      "Accuracy: 0.6076\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "input_size = model_cust.vector_size\n",
    "output_size = 3  # Binary classification\n",
    "model = CNN(input_size, output_size)\n",
    "\n",
    "# Define Loss Function and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "train_cnn(model, train_loader, criterion, optimizer, num_epochs=10)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_cnn(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
